{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the StarNet Model\n",
    "\n",
    "This notebook takes you through the steps of how to train a StarNet Model\n",
    "- Required Python packages: `numpy h5py keras`\n",
    "- Required data files: training_data.h5, mean_and_std.npy\n",
    "\n",
    "Note: We use tensorflow for the keras backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, InputLayer, Flatten, Reshape\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import HDF5Matrix\n",
    "\n",
    "datadir = \"\"\n",
    "training_set = datadir + 'training_data.h5'\n",
    "normalization_data = datadir + 'mean_and_std.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Normalization **\n",
    "\n",
    "Write a function to normalize the output labels. Each label will be normalized to have approximately have a mean of zero and unit variance.\n",
    "\n",
    "NOTE: This is necessary to put output labels on a similar scale in order for the model to train properly, this process is reversed in the test stage to give the output labels their proper units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_and_std = np.load(normalization_data)\n",
    "mean_labels = mean_and_std[0]\n",
    "std_labels = mean_and_std[1]\n",
    "\n",
    "def normalize(labels):\n",
    "    # Normalize labels\n",
    "    return (labels-mean_labels) / std_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Obtain training data **\n",
    "\n",
    "Here we will collect the output labels for the training and cross-validation sets, then normalize each.\n",
    "\n",
    "Next we will create an HDF5Matrix for the training and cross-validation input spectra rather than loading them all into memory. This is useful to save RAM when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each spectrum contains 7214 wavelength bins\n",
      "Training set includes 41000 spectra and the cross-validation set includes 3784 spectra\n"
     ]
    }
   ],
   "source": [
    "# Define the number of output labels\n",
    "num_labels = np.load(datadir+'mean_and_std.npy').shape[1]\n",
    "\n",
    "# Define the number of training spectra\n",
    "num_train = 41000\n",
    "\n",
    "# Load labels\n",
    "with  h5py.File(training_set, 'r') as F:\n",
    "    y_train = np.hstack((F['TEFF'][0:num_train], F['LOGG'][0:num_train], F['FE_H'][0:num_train]))\n",
    "    y_cv = np.hstack((F['TEFF'][num_train:], F['LOGG'][num_train:], F['FE_H'][num_train:]))\n",
    "\n",
    "# Normalize labels\n",
    "y_train = normalize(y_train)\n",
    "y_cv = normalize(y_cv)\n",
    "\n",
    "# Create the spectra training and cv datasets\n",
    "x_train = HDF5Matrix(training_set, 'spectrum', \n",
    "                           start=0, end=num_train)\n",
    "x_cv = HDF5Matrix(training_set, 'spectrum', \n",
    "                           start=num_train, end=None)\n",
    "\n",
    "# Define the number of output labels\n",
    "num_labels = y_train.shape[1]\n",
    "\n",
    "num_fluxes = x_train.shape[1]\n",
    "\n",
    "print('Each spectrum contains ' + str(num_fluxes) + ' wavelength bins')\n",
    "print('Training set includes ' + str(x_train.shape[0]) + \n",
    "      ' spectra and the cross-validation set includes ' + str(x_cv.shape[0])+' spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the StarNet model architecture**\n",
    "\n",
    "The StarNet architecture is built with:\n",
    "- input layer\n",
    "- 2 convolutional layers\n",
    "- 1 maxpooling layer followed by flattening for the fully connected layer\n",
    "- 2 fully connected layers\n",
    "- output layer\n",
    "\n",
    "First, let's define some model variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function used following every layer except for the output layers\n",
    "activation = 'relu'\n",
    "\n",
    "# model weight initializer\n",
    "initializer = 'he_normal'\n",
    "\n",
    "# number of filters used in the convolutional layers\n",
    "num_filters = [4,16]\n",
    "\n",
    "# length of the filters in the convolutional layers\n",
    "filter_length = 8\n",
    "\n",
    "# length of the maxpooling window \n",
    "pool_length = 4\n",
    "\n",
    "# number of nodes in each of the hidden fully connected layers\n",
    "num_hidden = [256,128]\n",
    "\n",
    "# number of spectra fed into model at once during training\n",
    "batch_size = 64\n",
    "\n",
    "# maximum number of interations for model training\n",
    "max_epochs = 30\n",
    "\n",
    "# initial learning rate for optimization algorithm\n",
    "lr = 0.0007\n",
    "    \n",
    "# exponential decay rate for the 1st moment estimates for optimization algorithm\n",
    "beta_1 = 0.9\n",
    "\n",
    "# exponential decay rate for the 2nd moment estimates for optimization algorithm\n",
    "beta_2 = 0.999\n",
    "\n",
    "# a small constant for numerical stability for optimization algorithm\n",
    "optimizer_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input spectra\n",
    "input_spec = Input(shape=(num_fluxes,), name='starnet_input_x')\n",
    "\n",
    "# Reshape spectra for CNN layers\n",
    "cur_in = Reshape((num_fluxes, 1))(input_spec)\n",
    "\n",
    "# CNN layers\n",
    "cur_in = Conv1D(kernel_initializer=initializer, activation=activation, \n",
    "                padding=\"same\", filters=num_filters[0], kernel_size=filter_length)(cur_in)\n",
    "cur_in = Conv1D(kernel_initializer=initializer, activation=activation,\n",
    "                padding=\"same\", filters=num_filters[1], kernel_size=filter_length)(cur_in)\n",
    "\n",
    "# Max pooling layer\n",
    "cur_in = MaxPooling1D(pool_size=pool_length)(cur_in)\n",
    "\n",
    "# Flatten the current input for the fully-connected layers\n",
    "cur_in = Flatten()(cur_in)\n",
    "\n",
    "# Fully-connected layers\n",
    "cur_in = Dense(units=num_hidden[0], kernel_initializer=initializer, \n",
    "               activation=activation)(cur_in)\n",
    "cur_in = Dense(units=num_hidden[1], kernel_initializer=initializer, \n",
    "               activation=activation)(cur_in)\n",
    "\n",
    "# Output nodes\n",
    "output_label = Dense(units=num_labels, activation=\"linear\", \n",
    "                     input_dim=num_hidden[1], name='starnet_output_y')(cur_in)\n",
    "\n",
    "model = Model(input_spec, output_label, name='StarNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More model techniques**\n",
    "* The `Adam` optimizer is the gradient descent algorithm used for minimizing the loss function\n",
    "* `EarlyStopping` uses the cross-validation set to test the model following every iteration and stops the training if the cv loss does not decrease by `min_delta` after `patience` iterations\n",
    "* `ReduceLROnPlateau` is a form of learning rate decay where the learning rate is decreased by a factor of `factor` if the training loss does not decrease by `epsilon` after `patience` iterations unless the learning rate has reached `min_lr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default loss function parameters\n",
    "early_stopping_min_delta = 0.0001\n",
    "early_stopping_patience = 4\n",
    "reduce_lr_factor = 0.5\n",
    "reuce_lr_epsilon = 0.0009\n",
    "reduce_lr_patience = 2\n",
    "reduce_lr_min = 0.00008\n",
    "\n",
    "# loss function to minimize\n",
    "loss_function = 'mean_squared_error'\n",
    "\n",
    "# compute mean absolute deviation\n",
    "metrics = ['mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=optimizer_epsilon, decay=0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=early_stopping_min_delta, \n",
    "                                       patience=early_stopping_patience, verbose=2, mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, epsilon=reuce_lr_epsilon, \n",
    "                                  patience=reduce_lr_patience, min_lr=reduce_lr_min, mode='min', verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "starnet_input_x (InputLayer) (None, 7214)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7214, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 7214, 4)           36        \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 7214, 16)          528       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1803, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 28848)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               7385344   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "starnet_output_y (Dense)     (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 7,419,191\n",
      "Trainable params: 7,419,191\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41000 samples, validate on 3784 samples\n",
      "Epoch 1/30\n",
      "41000/41000 [==============================] - 209s 5ms/step - loss: 0.1755 - mean_absolute_error: 0.2274 - val_loss: 0.0316 - val_mean_absolute_error: 0.1300\n",
      "Epoch 2/30\n",
      "41000/41000 [==============================] - 209s 5ms/step - loss: 0.0262 - mean_absolute_error: 0.1148 - val_loss: 0.0141 - val_mean_absolute_error: 0.0880\n",
      "Epoch 3/30\n",
      "41000/41000 [==============================] - 211s 5ms/step - loss: 0.0140 - mean_absolute_error: 0.0843 - val_loss: 0.0098 - val_mean_absolute_error: 0.0676\n",
      "Epoch 4/30\n",
      "41000/41000 [==============================] - 210s 5ms/step - loss: 0.0112 - mean_absolute_error: 0.0738 - val_loss: 0.0089 - val_mean_absolute_error: 0.0667\n",
      "Epoch 5/30\n",
      "41000/41000 [==============================] - 121s 3ms/step - loss: 0.0091 - mean_absolute_error: 0.0668 - val_loss: 0.0079 - val_mean_absolute_error: 0.0645\n",
      "Epoch 6/30\n",
      "41000/41000 [==============================] - 138s 3ms/step - loss: 0.0084 - mean_absolute_error: 0.0635 - val_loss: 0.0062 - val_mean_absolute_error: 0.0533\n",
      "Epoch 7/30\n",
      "41000/41000 [==============================] - 136s 3ms/step - loss: 0.0073 - mean_absolute_error: 0.0590 - val_loss: 0.0068 - val_mean_absolute_error: 0.0590\n",
      "Epoch 8/30\n",
      "41000/41000 [==============================] - 137s 3ms/step - loss: 0.0078 - mean_absolute_error: 0.0594 - val_loss: 0.0073 - val_mean_absolute_error: 0.0550\n",
      "Epoch 9/30\n",
      "41000/41000 [==============================] - 137s 3ms/step - loss: 0.0061 - mean_absolute_error: 0.0544 - val_loss: 0.0065 - val_mean_absolute_error: 0.0539\n",
      "Epoch 10/30\n",
      "41000/41000 [==============================] - 136s 3ms/step - loss: 0.0057 - mean_absolute_error: 0.0528 - val_loss: 0.0050 - val_mean_absolute_error: 0.0483\n",
      "Epoch 11/30\n",
      "41000/41000 [==============================] - 136s 3ms/step - loss: 0.0051 - mean_absolute_error: 0.0499 - val_loss: 0.0050 - val_mean_absolute_error: 0.0507\n",
      "Epoch 12/30\n",
      "41000/41000 [==============================] - 135s 3ms/step - loss: 0.0047 - mean_absolute_error: 0.0479 - val_loss: 0.0044 - val_mean_absolute_error: 0.0445\n",
      "Epoch 13/30\n",
      "41000/41000 [==============================] - 105s 3ms/step - loss: 0.0041 - mean_absolute_error: 0.0460 - val_loss: 0.0042 - val_mean_absolute_error: 0.0431\n",
      "Epoch 14/30\n",
      "41000/41000 [==============================] - 107s 3ms/step - loss: 0.0039 - mean_absolute_error: 0.0449 - val_loss: 0.0046 - val_mean_absolute_error: 0.0462\n",
      "Epoch 15/30\n",
      "41000/41000 [==============================] - 108s 3ms/step - loss: 0.0036 - mean_absolute_error: 0.0432 - val_loss: 0.0042 - val_mean_absolute_error: 0.0453\n",
      "Epoch 16/30\n",
      "41000/41000 [==============================] - 105s 3ms/step - loss: 0.0035 - mean_absolute_error: 0.0428 - val_loss: 0.0033 - val_mean_absolute_error: 0.0401\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n",
      "Epoch 17/30\n",
      "41000/41000 [==============================] - 101s 2ms/step - loss: 0.0027 - mean_absolute_error: 0.0368 - val_loss: 0.0034 - val_mean_absolute_error: 0.0405\n",
      "Epoch 18/30\n",
      "41000/41000 [==============================] - 104s 3ms/step - loss: 0.0025 - mean_absolute_error: 0.0359 - val_loss: 0.0028 - val_mean_absolute_error: 0.0357\n",
      "Epoch 19/30\n",
      "41000/41000 [==============================] - 145s 4ms/step - loss: 0.0024 - mean_absolute_error: 0.0354 - val_loss: 0.0029 - val_mean_absolute_error: 0.0366\n",
      "Epoch 20/30\n",
      "41000/41000 [==============================] - 146s 4ms/step - loss: 0.0024 - mean_absolute_error: 0.0353 - val_loss: 0.0031 - val_mean_absolute_error: 0.0388\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.00017499999376013875.\n",
      "Epoch 21/30\n",
      "41000/41000 [==============================] - 145s 4ms/step - loss: 0.0019 - mean_absolute_error: 0.0317 - val_loss: 0.0027 - val_mean_absolute_error: 0.0361\n",
      "Epoch 22/30\n",
      "41000/41000 [==============================] - 145s 4ms/step - loss: 0.0019 - mean_absolute_error: 0.0316 - val_loss: 0.0028 - val_mean_absolute_error: 0.0366\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 8.749999688006938e-05.\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf95ecfb00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_cv, y_cv),\n",
    "          epochs=max_epochs, verbose=1, shuffle='batch',\n",
    "          callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starnet_cnn.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "starnet_model = 'starnet_cnn.h5'\n",
    "model.save(datadir + starnet_model)\n",
    "print(starnet_model+' saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
