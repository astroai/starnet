{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "This notebook takes you through the steps of how to train a StarNet Model.\n",
    "\n",
    "IMPORTANT: If you do not have access to a sufficient computing power (ie. Virtual Machines, GPU, etc.) use this notebook instead of $4\\_Train\\_Model\\_Keras\\_1.ipynb$. The only difference between the two is that this notebook will use fewer training examples in attempt to decrease the computational load and will require more training iterations to compensate for this. It should be noted that using fewer training examples will likely result in less precise predictions during the test phase.\n",
    "\n",
    "## required packages:\n",
    "- numpy\n",
    "- h5py\n",
    "- random\n",
    "- Keras 1\n",
    "  - for Keras 2 users, see $3\\_Train\\_Model\\_Keras\\_2.ipynb$\n",
    "- Tensorflow or Theano (backend for Keras)\n",
    "\n",
    "NOTE: for error propagation, the model must be trained with Theano as the backend, although this has been found to be much slower without the proper set-up of Theano\n",
    "\n",
    "## required data files:\n",
    "- training_data.h5\n",
    "  - can be created in $2\\_Preprocessing\\_of\\_Training\\_Data.ipynb$ or downloaded  in $1\\_Download\\_Data.ipynb$\n",
    "- mean_and_std.npy\n",
    "  - can be created in $3\\_Preprocessing\\_of\\_Test\\_Data.ipynb$ or downloaded in $1\\_Download\\_Data.ipynb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for normalizing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_and_std = np.load('mean_and_std.npy')\n",
    "mean_labels = mean_and_std[0]\n",
    "std_labels = mean_and_std[1]\n",
    "num_labels = mean_and_std.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function to normalize labels to approximately have a mean of zero and unit variance\n",
    "NOTE: this is necessary to put output labels on a similar scale in order for the model to train properly, this process is reversed in the test stage to give the output labels their proper units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(lb):\n",
    "    return (lb-mean_labels)/std_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain reference set\n",
    "### Default:\n",
    "num_ref = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savename = 'training_data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_ref = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference set includes 10000 individual visit spectra.\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(savename,\"r\") as F:\n",
    "    spectra = F[\"spectra\"][0:num_ref]\n",
    "    labels = np.column_stack((F[\"TEFF\"][0:num_ref],F[\"LOGG\"][0:num_ref],F[\"FE_H\"][0:num_ref]))\n",
    "    # Normalize labels\n",
    "    labels = normalize(labels)\n",
    "print('Reference set includes '+str(len(spectra))+' individual visit spectra.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each spectra contains 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# define the number of wavelength bins (typicall 7214)\n",
    "num_fluxes = spectra.shape[1]\n",
    "print('Each spectra contains '+str(num_fluxes)+' wavelength bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomize order and separate into training and CV sets\n",
    "This is done to randomize order for proper training\n",
    "The data is then split the reference set into training and cross-validation sets\n",
    "### Default:\n",
    "- $num\\_train$ = 90% of reference set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train=int(0.9*num_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set includes 9000 spectra and the cross-validation set includes 1000 spectra\n"
     ]
    }
   ],
   "source": [
    "reference_data = np.column_stack((spectra,labels))\n",
    "np.random.shuffle(reference_data)\n",
    "\n",
    "train_spectra = reference_data[0:num_train,0:num_fluxes]\n",
    "# Reshape spectra for convolutional layers\n",
    "train_spectra = train_spectra.reshape(train_spectra.shape[0], train_spectra.shape[1], 1)\n",
    "train_labels = reference_data[0:num_train,num_fluxes:]\n",
    "\n",
    "cv_spectra = reference_data[num_train:,0:num_fluxes]\n",
    "cv_spectra = cv_spectra.reshape(cv_spectra.shape[0], cv_spectra.shape[1], 1)\n",
    "cv_labels = reference_data[num_train:,num_fluxes:]\n",
    "\n",
    "print('Training set includes '+str(len(train_spectra))+' spectra and the cross-validation set includes '+str(len(cv_spectra))+' spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# activation function used following every layer except for the output layers\n",
    "activation = 'relu'\n",
    "\n",
    "# model weight initializer\n",
    "initializer = 'he_normal'\n",
    "\n",
    "# shape of input spectra that is fed into the input layer\n",
    "input_shape = (None,num_fluxes,1)\n",
    "\n",
    "# number of filters used in the convolutional layers\n",
    "num_filters = [4,16]\n",
    "\n",
    "# length of the filters in the convolutional layers\n",
    "filter_length = 8\n",
    "\n",
    "# length of the maxpooling window \n",
    "pool_length = 4\n",
    "\n",
    "# number of nodes in each of the hidden fully connected layers\n",
    "num_hidden_nodes = [256,128]\n",
    "\n",
    "# number of spectra fed into model at once during training\n",
    "batch_size = 64\n",
    "\n",
    "# maximum number of interations for model training\n",
    "max_epochs = 45\n",
    "\n",
    "# initial learning rate for optimization algorithm\n",
    "lr = 0.0007\n",
    "    \n",
    "# exponential decay rate for the 1st moment estimates for optimization algorithm\n",
    "beta_1 = 0.9\n",
    "\n",
    "# exponential decay rate for the 2nd moment estimates for optimization algorithm\n",
    "beta_2 = 0.999\n",
    "\n",
    "# a small constant for numerical stability for optimization algorithm\n",
    "optimizer_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Build Model Architecture:\n",
    "- input layer\n",
    "- 2 convolutional layers\n",
    "- 1 maxpooling layer followed by flattening for the fully connected layer\n",
    "- 2 fully connected layers\n",
    "- output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "\n",
    "    InputLayer(batch_input_shape=input_shape),\n",
    "        \n",
    "    Conv1D(init=initializer, activation=activation, border_mode=\"same\", nb_filter=num_filters[0], filter_length=filter_length),\n",
    "\n",
    "    Conv1D(init=initializer, activation=activation, border_mode=\"same\", nb_filter=num_filters[1], filter_length=filter_length),\n",
    "\n",
    "    MaxPooling1D(pool_length=pool_length),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(output_dim=num_hidden_nodes[0], init=initializer, activation=activation),\n",
    "        \n",
    "    Dense(output_dim=num_hidden_nodes[1], init=initializer, activation=activation),\n",
    "\n",
    "    Dense(output_dim=num_labels, activation=\"linear\", input_dim=num_hidden_nodes[1]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More model techniques:\n",
    "- The $Adam$ optimizer is the gradient descent algorithm used for minimizing the loss function\n",
    "- $EarlyStopping$ uses the cross-validation set to test the model following every iteration and stops the training if the cv loss does not decrease by $min\\_delta$ after $patience$ iterations\n",
    "- $ReduceLROnPlateau$ is a form of learning rate decay where the learning rate is decreased by a factor of $factor$ if the training loss does not decrease by $epsilon$ after $patience$ iterations unless the learning rate has reached $min\\_lr$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping_min_delta = 0.0001\n",
    "early_stopping_patience = 4\n",
    "reduce_lr_factor = 0.5\n",
    "reuce_lr_epsilon = 0.0009\n",
    "reduce_lr_patience = 2\n",
    "reduce_lr_min = 0.00008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=optimizer_epsilon, decay=0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=early_stopping_min_delta, \n",
    "                                       patience=early_stopping_patience, verbose=2, mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, epsilon=reuce_lr_epsilon, \n",
    "                                  patience=reduce_lr_patience, min_lr=reduce_lr_min, mode='min', verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compile model\n",
    "### Default loss function:\n",
    "- loss_function = mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = 'mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/45\n",
      "19s - loss: 0.6099 - val_loss: 0.5748\n",
      "Epoch 2/45\n",
      "19s - loss: 0.2165 - val_loss: 0.0574\n",
      "Epoch 3/45\n",
      "19s - loss: 0.0498 - val_loss: 0.0270\n",
      "Epoch 4/45\n",
      "19s - loss: 0.0302 - val_loss: 0.0206\n",
      "Epoch 5/45\n",
      "19s - loss: 0.0335 - val_loss: 0.0155\n",
      "Epoch 6/45\n",
      "19s - loss: 0.0154 - val_loss: 0.0196\n",
      "Epoch 7/45\n",
      "19s - loss: 0.0182 - val_loss: 0.0207\n",
      "Epoch 8/45\n",
      "19s - loss: 0.0154 - val_loss: 0.0121\n",
      "Epoch 9/45\n",
      "20s - loss: 0.0109 - val_loss: 0.0106\n",
      "Epoch 10/45\n",
      "20s - loss: 0.0126 - val_loss: 0.0146\n",
      "Epoch 11/45\n",
      "20s - loss: 0.0117 - val_loss: 0.0099\n",
      "Epoch 12/45\n",
      "19s - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 13/45\n",
      "19s - loss: 0.0051 - val_loss: 0.0071\n",
      "Epoch 14/45\n",
      "19s - loss: 0.0046 - val_loss: 0.0071\n",
      "Epoch 15/45\n",
      "20s - loss: 0.0045 - val_loss: 0.0075\n",
      "Epoch 16/45\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.00034999998752.\n",
      "19s - loss: 0.0047 - val_loss: 0.0082\n",
      "Epoch 17/45\n",
      "20s - loss: 0.0041 - val_loss: 0.0062\n",
      "Epoch 18/45\n",
      "19s - loss: 0.0031 - val_loss: 0.0062\n",
      "Epoch 19/45\n",
      "20s - loss: 0.0029 - val_loss: 0.0061\n",
      "Epoch 20/45\n",
      "19s - loss: 0.0030 - val_loss: 0.0065\n",
      "Epoch 21/45\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.00017499999376.\n",
      "19s - loss: 0.0030 - val_loss: 0.0059\n",
      "Epoch 22/45\n",
      "19s - loss: 0.0026 - val_loss: 0.0058\n",
      "Epoch 23/45\n",
      "\n",
      "Epoch 00022: reducing learning rate to 8.74999968801e-05.\n",
      "19s - loss: 0.0025 - val_loss: 0.0057\n",
      "Epoch 24/45\n",
      "20s - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 25/45\n",
      "\n",
      "Epoch 00024: reducing learning rate to 8e-05.\n",
      "20s - loss: 0.0024 - val_loss: 0.0057\n",
      "Epoch 26/45\n",
      "19s - loss: 0.0023 - val_loss: 0.0058\n",
      "Epoch 27/45\n",
      "19s - loss: 0.0023 - val_loss: 0.0056\n",
      "Epoch 28/45\n",
      "20s - loss: 0.0023 - val_loss: 0.0058\n",
      "Epoch 29/45\n",
      "19s - loss: 0.0023 - val_loss: 0.0056\n",
      "Epoch 30/45\n",
      "19s - loss: 0.0022 - val_loss: 0.0056\n",
      "Epoch 31/45\n",
      "19s - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 32/45\n",
      "19s - loss: 0.0022 - val_loss: 0.0056\n",
      "Epoch 00031: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f807d50ff10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_spectra, train_labels, validation_data=(cv_spectra, cv_labels),\n",
    "          nb_epoch=max_epochs, batch_size=batch_size, verbose=2,\n",
    "          callbacks=[reduce_lr,early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as Model_0.h5\n"
     ]
    }
   ],
   "source": [
    "savename = 'Model_0'\n",
    "model.save(savename+'.h5')\n",
    "print('Model saved as '+savename+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
