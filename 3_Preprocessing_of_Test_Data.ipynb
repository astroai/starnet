{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process test data\n",
    "\n",
    "This notebook takes you through the steps of how to preprocess a high S/N and low S/N test set\n",
    "* required packages: numpy, h5py\n",
    "* required data files: apStar_combined_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import vos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apStar_combined_main.h5 downloaded\n"
     ]
    }
   ],
   "source": [
    "datadir='/home/ubuntu/starnet_data/'  # or \"/path/to/my/starnet/directory\"\n",
    "\n",
    "def starnet_download_file(filename):\n",
    "    vclient = vos.Client()\n",
    "    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\n",
    "    print(filename+' downloaded')\n",
    "\n",
    "starnet_download_file('apStar_combined_main.h5')\n",
    "f = h5py.File(datadir+'apStar_combined_main.h5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n",
      "0_H\n",
      "0_H_ERR\n",
      "ALPHA_M\n",
      "AL_H\n",
      "AL_H_ERR\n",
      "CA_H\n",
      "CA_H_ERR\n",
      "C_H\n",
      "C_H_ERR\n",
      "FE_H\n",
      "FE_H_ERR\n",
      "IDs\n",
      "K_H\n",
      "K_H_ERR\n",
      "LOGG\n",
      "LOGG_ERR\n",
      "MG_H\n",
      "MG_H_ERR\n",
      "MN_H\n",
      "MN_H_ERR\n",
      "NA_H\n",
      "NA_H_ERR\n",
      "NI_H\n",
      "NI_H_ERR\n",
      "N_H\n",
      "N_H_ERR\n",
      "SI_H\n",
      "SI_H_ERR\n",
      "S_H\n",
      "S_H_ERR\n",
      "TEFF\n",
      "TEFF_ERR\n",
      "TI_H\n",
      "TI_H_ERR\n",
      "VRAD\n",
      "VRAD_ERR\n",
      "VSCATTER\n",
      "V_H\n",
      "V_H_ERR\n",
      "aspcap_flag\n",
      "error_spectrum\n",
      "num_visits\n",
      "spectrum\n",
      "stacked_snr\n",
      "star_flag\n",
      "targ1_flag\n",
      "targ2_flag\n"
     ]
    }
   ],
   "source": [
    "print('Dataset keys in file: \\n')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data into memory**\n",
    "\n",
    "For the testing of StarNet, it is necessary to obtain the spectra, error spectra, combined S/N, and labels, but we need to make eliminations to the test set to obtain the labels of highest validity to compare with, so we will first include the APOGEE_IDs, the spectra, error spectra, the S/N of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$, [Fe/H], $V_{scatter}$, STARFLAGs, and ASPCAPFLAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtainined spectra and data for 142333 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = f['IDs'][:,0]\n",
    "\n",
    "spectra = f['spectrum'][:]\n",
    "error_spectra = f['error_spectrum'][:]\n",
    "combined_snr = f['stacked_snr'][:]\n",
    "starflag = f['star_flag'][:]\n",
    "aspcapflag = f['aspcap_flag'][:]\n",
    "teff = f['TEFF'][:]\n",
    "logg = f['LOGG'][:]\n",
    "fe_h = f['FE_H'][:]\n",
    "vscatter = f['VSCATTER'][:]\n",
    "\n",
    "print('Obtainined spectra and data for '+str(len(list(set(list(ap_id)))))+' stars.')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the data**\n",
    "\n",
    "Create a file that contains the mean and standard deviation for $T_{\\mathrm{eff}}$, $\\log(g)$, and  [Fe/H] in order to normalize labels during training and testing ignore values equal to -9999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_and_std.npy saved\n"
     ]
    }
   ],
   "source": [
    "mean = np.array([np.mean(teff[teff!=-9999.]),np.mean(logg[logg!=-9999.]),np.mean(fe_h[fe_h!=-9999.])])\n",
    "std = np.array([np.std(teff[teff!=-9999.]),np.std(logg[logg!=-9999.]),np.std(fe_h[fe_h!=-9999.])])\n",
    "mean_and_std = np.row_stack((mean,std))\n",
    "np.save(datadir+'mean_and_std', mean_and_std)\n",
    "\n",
    "print('mean_and_std.npy saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate out a dataset with good labels**. \n",
    "- STARFLAGs = 0\n",
    "- ASPCAPFLAGs = 0\n",
    "- 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "- -3.0 < [Fe/H]\n",
    "- $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "- $V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34484 stars remain.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "ap_id = ap_id[indices]\n",
    "spectra = spectra[indices]\n",
    "error_spectra = error_spectra[indices]\n",
    "teff = teff[indices]\n",
    "logg = logg[indices]\n",
    "fe_h = fe_h[indices]\n",
    "combined_snr = combined_snr[indices]\n",
    "\n",
    "print(str(len(list(set(list(ap_id)))))+' stars remain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load high S/N APOGEE IDs**\n",
    "\n",
    "Load a file that contains the APOGEE IDs for High S/N spectra that will be processed into the High S/N test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_snr_test_ap_ids = np.load('high_snr_test_apids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate data for High S/N test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High S/N test set includes 2780 combined spectra\n"
     ]
    }
   ],
   "source": [
    "indices = [i for i, item in enumerate(high_snr_test_ap_ids) if item in ap_id]\n",
    "\n",
    "high_snr_ap_id = ap_id[indices]\n",
    "high_snr_spectra = spectra[indices]\n",
    "high_snr_error_spectra = error_spectra[indices]\n",
    "high_snr_teff = teff[indices]\n",
    "high_snr_logg = logg[indices]\n",
    "high_snr_fe_h = fe_h[indices]\n",
    "high_snr_combined_snr = combined_snr[indices]\n",
    "\n",
    "print('High S/N test set includes '+str(len(high_snr_ap_id))+' combined spectra')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Normalize spectra**:\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values\n",
    "4. Error spectra must also be normalized with the same median values for error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High S/N spectra dataset now contains 2780 spectra, each with 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# Separate spectra into chips\n",
    "\n",
    "blue_sp = high_snr_spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = high_snr_spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = high_snr_spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "#Normalize spectra by chips\n",
    "\n",
    "blue_sp_med = np.median(blue_sp, axis=1)\n",
    "green_sp_med = np.median(green_sp, axis=1)\n",
    "red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "green_sp = (green_sp.T/green_sp_med).T\n",
    "red_sp = (red_sp.T/red_sp_med).T  \n",
    "\n",
    "# Recombine spectra\n",
    "\n",
    "high_snr_spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "\n",
    "print('High S/N spectra dataset now contains '+str(high_snr_spectra.shape[0])+' spectra, each with '+str(high_snr_spectra.shape[1])+' wavelength bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High S/N error spectra dataset now contains 2780 error spectra, each with 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# Separate error spectra into chips\n",
    "\n",
    "blue_sp = high_snr_error_spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = high_snr_error_spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = high_snr_error_spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "# Normalize error spectra by chips\n",
    "\n",
    "blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "green_sp = (green_sp.T/green_sp_med).T\n",
    "red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "# Recombine error spectra\n",
    "\n",
    "high_snr_error_spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "\n",
    "print('High S/N error spectra dataset now contains '+str(high_snr_error_spectra.shape[0])+' error spectra, each with '+str(high_snr_error_spectra.shape[1])+' wavelength bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the new High S/N test data file**\n",
    "\n",
    "with APOGEE IDs, spectra, error spectra, combined S/N and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_snr_test_data.h5 has been saved as the High S/N test set to be used in 5_Test_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = datadir + 'high_snr_test_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(savename) \n",
    "#os.remove(savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "with h5py.File(savename, \"a\") as f:\n",
    "     \n",
    "    spectra_ds = f.create_dataset('spectra', high_snr_spectra.shape, dtype=\"f\")\n",
    "    error_spectra_ds = f.create_dataset('error_spectra', high_snr_error_spectra.shape, dtype=\"f\")\n",
    "    teff_ds = f.create_dataset('TEFF', high_snr_teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', high_snr_logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', high_snr_fe_h.shape, dtype=\"f\")\n",
    "    combined_snr_ds = f.create_dataset('combined_snr', high_snr_combined_snr.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_IDs', high_snr_ap_id.shape, dtype=\"S18\")\n",
    "    \n",
    "    spectra_ds[:] = high_snr_spectra\n",
    "    error_spectra_ds[:] = high_snr_error_spectra\n",
    "    teff_ds[:] = high_snr_teff\n",
    "    logg_ds[:] = high_snr_logg\n",
    "    fe_h_ds[:] = high_snr_fe_h\n",
    "    combined_snr_ds[:] = high_snr_combined_snr\n",
    "    ap_id_ds[:] = high_snr_ap_id.tolist()\n",
    "    \n",
    "print(savename+' has been saved as the High S/N test set to be used in 5_Test_Model.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a low S/N test set**\n",
    "\n",
    "1. Add a cut to combined S/N < 200\n",
    "2. Normalize the spectra as before\n",
    "3. Save the spectra just like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snr_max = 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low S/N test set includes 17506 combined spectra\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((combined_snr[:]<snr_max).reshape(len(ap_id),1))\n",
    "\n",
    "low_snr_ap_id = ap_id[indices]\n",
    "low_snr_spectra = spectra[indices]\n",
    "low_snr_error_spectra = error_spectra[indices]\n",
    "low_snr_teff = teff[indices]\n",
    "low_snr_logg = logg[indices]\n",
    "low_snr_fe_h = fe_h[indices]\n",
    "low_snr_combined_snr = combined_snr[indices]\n",
    "\n",
    "print('Low S/N test set includes '+str(len(low_snr_ap_id))+' combined spectra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low S/N spectra dataset now contains 17506 spectra, each with 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# Separate spectra into chips/home/ubuntu/conda/lib/python2.7/site-packages/keras/metrics.py\n",
    "\n",
    "blue_sp = low_snr_spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = low_snr_spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = low_snr_spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "#Normalize spectra by chips\n",
    "\n",
    "blue_sp_med = np.median(blue_sp, axis=1)\n",
    "green_sp_med = np.median(green_sp, axis=1)\n",
    "red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "green_sp = (green_sp.T/green_sp_med).T\n",
    "red_sp = (red_sp.T/red_sp_med).T  \n",
    "\n",
    "# Recombine spectra\n",
    "\n",
    "low_snr_spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "print('Low S/N spectra dataset now contains '+str(low_snr_spectra.shape[0])+' spectra, each with '+str(low_snr_spectra.shape[1])+' wavelength bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low S/N error spectra dataset now contains 17506 error spectra, each with 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# Separate error spectra into chips\n",
    "\n",
    "blue_sp = low_snr_error_spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = low_snr_error_spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = low_snr_error_spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "# Normalize error spectra by chips\n",
    "\n",
    "blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "green_sp = (green_sp.T/green_sp_med).T\n",
    "red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "# Recombine error spectra\n",
    "\n",
    "low_snr_error_spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "\n",
    "print('Low S/N error spectra dataset now contains '+str(low_snr_error_spectra.shape[0])+' error spectra, each with '+str(low_snr_error_spectra.shape[1])+' wavelength bins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_snr_test_data.h5 has been saved as the Low S/N test set to be used in 5_Test_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = datadir+'low_snr_test_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(savename) \n",
    "#os.remove(savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "with h5py.File(savename, \"a\") as f:\n",
    "     \n",
    "    spectra_ds = f.create_dataset('spectra', low_snr_spectra.shape, dtype=\"f\")\n",
    "    error_spectra_ds = f.create_dataset('error_spectra', low_snr_error_spectra.shape, dtype=\"f\")\n",
    "    teff_ds = f.create_dataset('TEFF', low_snr_teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', low_snr_logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', low_snr_fe_h.shape, dtype=\"f\")\n",
    "    combined_snr_ds = f.create_dataset('combined_snr', low_snr_combined_snr.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_IDs', low_snr_ap_id.shape, dtype=\"S18\")\n",
    "    \n",
    "    spectra_ds[:] = low_snr_spectra\n",
    "    error_spectra_ds[:] = low_snr_error_spectra\n",
    "    teff_ds[:] = low_snr_teff\n",
    "    logg_ds[:] = low_snr_logg\n",
    "    fe_h_ds[:] = low_snr_fe_h\n",
    "    combined_snr_ds[:] = low_snr_combined_snr\n",
    "    ap_id_ds[:] = low_snr_ap_id.tolist()\n",
    "\n",
    "print(savename + ' has been saved as the Low S/N test set to be used in 5_Test_Model.ipynb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
