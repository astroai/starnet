{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process test data\n",
    "\n",
    "This notebook takes you through the steps of how to preprocess a high S/N and low S/N test set\n",
    "* required packages: numpy, h5py, vos\n",
    "* required data files: apStar_combined_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import vos\n",
    "\n",
    "datadir='/home/ubuntu/starnet_data/'  # or \"/path/to/my/starnet/directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If you have not downloaded apStar_combined_main.h5 uncomment the below code to copy the file **\n",
    "\n",
    "Note: This file requires 10.3GB. It is necessary to download this file to run  particular notebook, although this notebook can be skipped by downloading the files created here seperately. See $1\\_Download\\_Data.ipynb$ for instructions on how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef starnet_download_file(filename):\\n    vclient = vos.Client()\\n    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\\n    print(filename+' downloaded')\\n\\nstarnet_download_file('apStar_combined_main.h5')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def starnet_download_file(filename):\n",
    "    vclient = vos.Client()\n",
    "    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\n",
    "    print(filename+' downloaded')\n",
    "\n",
    "starnet_download_file('apStar_combined_main.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0_FE',\n",
       " u'0_FE_ERR',\n",
       " u'ALPHA_M',\n",
       " u'AL_FE',\n",
       " u'AL_FE_ERR',\n",
       " u'CA_FE',\n",
       " u'CA_FE_ERR',\n",
       " u'C_FE',\n",
       " u'C_FE_ERR',\n",
       " u'FE_H',\n",
       " u'FE_H_ERR',\n",
       " u'IDs',\n",
       " u'K_FE',\n",
       " u'K_FE_ERR',\n",
       " u'LOGG',\n",
       " u'LOGG_ERR',\n",
       " u'MG_FE',\n",
       " u'MG_FE_ERR',\n",
       " u'MN_FE',\n",
       " u'MN_FE_ERR',\n",
       " u'NA_FE',\n",
       " u'NA_FE_ERR',\n",
       " u'NI_FE',\n",
       " u'NI_FE_ERR',\n",
       " u'N_FE',\n",
       " u'N_FE_ERR',\n",
       " u'PARAM',\n",
       " u'SI_FE',\n",
       " u'SI_FE_ERR',\n",
       " u'S_FE',\n",
       " u'S_FE_ERR',\n",
       " u'TEFF',\n",
       " u'TEFF_ERR',\n",
       " u'TI_FE',\n",
       " u'TI_FE_ERR',\n",
       " u'VRAD',\n",
       " u'VRAD_ERR',\n",
       " u'VSCATTER',\n",
       " u'V_FE',\n",
       " u'V_FE_ERR',\n",
       " u'aspcap_flag',\n",
       " u'error_spectrum',\n",
       " u'num_visits',\n",
       " u'spectrum',\n",
       " u'stacked_snr',\n",
       " u'star_flag',\n",
       " u'targ1_flag',\n",
       " u'targ2_flag']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filename = datadir + 'apStar_combined_main.h5'\n",
    "filename = datadir + 'apStar_combined_main_dr13.h5'\n",
    "F = h5py.File(filename,'r')\n",
    "print('Dataset keys in file: \\n')\n",
    "F.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data into memory**\n",
    "\n",
    "For the testing of StarNet, it is necessary to obtain the spectra, error spectra, combined S/N, and labels, but we need to make eliminations to the test set to obtain the labels of highest validity to compare with, so we will first include the APOGEE_IDs, the S/N of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$, [Fe/H], $V_{scatter}$, STARFLAGs, and ASPCAPFLAGs to make certain eliminations. Once the stars for the test sets have been collected we will then gather the spectra and error spectra and save the two test sets to an h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtainined data for 143482 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = F['IDs'][:,0]\n",
    "combined_snr = F['stacked_snr'][:]\n",
    "starflag = F['star_flag']\n",
    "aspcapflag = F['aspcap_flag']\n",
    "teff = F['TEFF'][:]\n",
    "logg = F['LOGG'][:]\n",
    "fe_h = F['FE_H'][:]\n",
    "vscatter = F['VSCATTER']\n",
    "\n",
    "print('Obtainined data for '+str(len(list(set(list(ap_id)))))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect label normalization data**\n",
    "\n",
    "Create a file that contains the mean and standard deviation for $T_{\\mathrm{eff}}$, $\\log(g)$, and  $[Fe/H]$ in order to normalize labels during training and testing. Ignore values equal to -9999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_and_std.npy saved\n"
     ]
    }
   ],
   "source": [
    "mean = np.array([np.mean(teff[teff!=-9999.]),np.mean(logg[logg!=-9999.]),np.mean(fe_h[fe_h!=-9999.])])\n",
    "std = np.array([np.std(teff[teff!=-9999.]),np.std(logg[logg!=-9999.]),np.std(fe_h[fe_h!=-9999.])])\n",
    "mean_and_std = np.row_stack((mean,std))\n",
    "np.save(datadir+'mean_and_std', mean_and_std)\n",
    "\n",
    "print('mean_and_std.npy saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate out a dataset with good labels**. \n",
    "- STARFLAGs = 0\n",
    "- ASPCAPFLAGs = 0\n",
    "- 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "- -3.0 < [Fe/H]\n",
    "- $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "- $V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35511 stars remain.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "ap_id = ap_id[indices]\n",
    "teff = teff[indices]\n",
    "logg = logg[indices]\n",
    "fe_h = fe_h[indices]\n",
    "combined_snr = combined_snr[indices]\n",
    "\n",
    "print(str(len(list(set(list(ap_id)))))+' stars remain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load high S/N APOGEE IDs**\n",
    "\n",
    "Load a file that contains the APOGEE IDs for High S/N spectra that will be processed into the High S/N test set. This file was created in 2_Preprocessing_of_Training_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_snr_test_ap_ids = np.load(datadir + 'high_snr_test_apids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate data for High S/N test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High S/N test set includes 2651 combined spectra\n"
     ]
    }
   ],
   "source": [
    "indices_high_snr = [i for i, item in enumerate(high_snr_test_ap_ids) if item in ap_id]\n",
    "\n",
    "high_snr_ap_id = ap_id[indices_high_snr]\n",
    "high_snr_teff = teff[indices_high_snr]\n",
    "high_snr_logg = logg[indices_high_snr]\n",
    "high_snr_fe_h = fe_h[indices_high_snr]\n",
    "high_snr_combined_snr = combined_snr[indices_high_snr]\n",
    "\n",
    "indices_high_snr = indices[indices_high_snr] # These indices will be used to index through the spectra\n",
    "\n",
    "print('High S/N test set includes '+str(len(high_snr_ap_id))+' combined spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Now collect spectra and error spectra. Then normalize each spectrum and save the data**\n",
    "\n",
    "**Steps taken to normalize spectra:**\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values\n",
    "4. Error spectra must also be normalized with the same median values for use in the error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_snr_test_data.h5 has been saved as the high S/N test set to be used in 5_test_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'high_snr_test_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(path) \n",
    "#os.remove(datadir + savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "\n",
    "with h5py.File(datadir + savename, \"a\") as f:\n",
    "    \n",
    "    # Create datasets for your high S/N data file \n",
    "    spectra_ds = f.create_dataset('spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    error_spectra_ds = f.create_dataset('error_spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    teff_ds = f.create_dataset('TEFF', high_snr_teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', high_snr_logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', high_snr_fe_h.shape, dtype=\"f\")\n",
    "    combined_snr_ds = f.create_dataset('combined_snr', high_snr_combined_snr.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_ID', high_snr_ap_id.shape, dtype=\"S18\")\n",
    "    \n",
    "    # Save data to data file\n",
    "    teff_ds[:] = high_snr_teff\n",
    "    logg_ds[:] = high_snr_logg\n",
    "    fe_h_ds[:] = high_snr_fe_h\n",
    "    combined_snr_ds[:] = high_snr_combined_snr\n",
    "    ap_id_ds[:] = high_snr_ap_id.tolist()\n",
    "        \n",
    "    # Collect spectra\n",
    "    first_entry=True\n",
    "    \n",
    "    for i in indices_high_snr:\n",
    "\n",
    "        spectrum = F['spectrum'][i:i+1]\n",
    "        err_spectrum = F['error_spectrum'][i:i+1]\n",
    "\n",
    "\n",
    "        # NORMALIZE SPECTRUM\n",
    "        # Separate spectra into chips\n",
    "        blue_sp = spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "        \n",
    "        blue_sp_med = np.median(blue_sp, axis=1)\n",
    "        green_sp_med = np.median(green_sp, axis=1)\n",
    "        red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "        #Normalize spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine spectra\n",
    "        spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        # Normalize error spectrum using the same method\n",
    "        # Separate error spectra into chips\n",
    "\n",
    "        blue_sp = err_spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = err_spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = err_spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "\n",
    "        # Normalize error spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine error spectra\n",
    "        err_spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        if first_entry:\n",
    "            spectra_ds[0] = spectrum\n",
    "            error_spectra_ds[0] = err_spectrum\n",
    "            first_entry=False\n",
    "        else:\n",
    "            spectra_ds.resize(spectra_ds.shape[0]+1, axis=0)\n",
    "            error_spectra_ds.resize(error_spectra_ds.shape[0]+1, axis=0)\n",
    "\n",
    "            spectra_ds[-1] = spectrum\n",
    "            error_spectra_ds[-1] = err_spectrum\n",
    "\n",
    "print(savename+' has been saved as the high S/N test set to be used in 5_test_Model.ipynb')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a low S/N test set**\n",
    "\n",
    "1. Add a cut to combined S/N < 200\n",
    "2. Normalize the spectra as before\n",
    "3. Save the data just like above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snr_max = 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low S/N test set includes 18386 combined spectra\n"
     ]
    }
   ],
   "source": [
    "indices_low_snr, cols = np.where((combined_snr[:]<snr_max).reshape(len(ap_id),1))\n",
    "\n",
    "low_snr_ap_id = ap_id[indices_low_snr]\n",
    "low_snr_teff = teff[indices_low_snr]\n",
    "low_snr_logg = logg[indices_low_snr]\n",
    "low_snr_fe_h = fe_h[indices_low_snr]\n",
    "low_snr_combined_snr = combined_snr[indices_low_snr]\n",
    "\n",
    "indices_low_snr = indices[indices_low_snr] # These indices will be used to index through the spectra\n",
    "\n",
    "print('Low S/N test set includes '+str(len(low_snr_ap_id))+' combined spectra')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low_snr_test_data.h5 has been saved as the low S/N test set to be used in 5_Test_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'low_snr_test_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(path) \n",
    "#os.remove(datadir + savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "\n",
    "with h5py.File(datadir + savename, \"a\") as f:\n",
    "    \n",
    "    # Create datasets for your high S/N data file \n",
    "    spectra_ds = f.create_dataset('spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    error_spectra_ds = f.create_dataset('error_spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    teff_ds = f.create_dataset('TEFF', low_snr_teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', low_snr_logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', low_snr_fe_h.shape, dtype=\"f\")\n",
    "    combined_snr_ds = f.create_dataset('combined_snr', low_snr_combined_snr.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_ID', low_snr_ap_id.shape, dtype=\"S18\")\n",
    "    \n",
    "    # Save data to data file\n",
    "    teff_ds[:] = low_snr_teff\n",
    "    logg_ds[:] = low_snr_logg\n",
    "    fe_h_ds[:] = low_snr_fe_h\n",
    "    combined_snr_ds[:] = low_snr_combined_snr\n",
    "    ap_id_ds[:] = low_snr_ap_id.tolist()\n",
    "        \n",
    "    # Collect spectra\n",
    "    first_entry=True\n",
    "    \n",
    "    for i in indices_low_snr:\n",
    "\n",
    "        spectrum = F['spectrum'][i:i+1]\n",
    "        err_spectrum = F['error_spectrum'][i:i+1]\n",
    "\n",
    "\n",
    "        # NORMALIZE SPECTRUM\n",
    "        # Separate spectra into chips\n",
    "        blue_sp = spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "        \n",
    "        blue_sp_med = np.median(blue_sp, axis=1)\n",
    "        green_sp_med = np.median(green_sp, axis=1)\n",
    "        red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "        #Normalize spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine spectra\n",
    "        spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        # Normalize error spectrum using the same method\n",
    "        # Separate error spectra into chips\n",
    "\n",
    "        blue_sp = err_spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = err_spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = err_spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "\n",
    "        # Normalize error spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine error spectra\n",
    "        err_spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        \n",
    "        if first_entry:\n",
    "            spectra_ds[0] = spectrum\n",
    "            error_spectra_ds[0] = err_spectrum\n",
    "            \n",
    "            first_entry=False\n",
    "        else:\n",
    "            spectra_ds.resize(spectra_ds.shape[0]+1, axis=0)\n",
    "            error_spectra_ds.resize(error_spectra_ds.shape[0]+1, axis=0)\n",
    "\n",
    "            spectra_ds[-1] = spectrum\n",
    "            error_spectra_ds[-1] = err_spectrum\n",
    "\n",
    "print(savename+' has been saved as the low S/N test set to be used in 5_Test_Model.ipynb')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
