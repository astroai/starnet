{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process test data\n",
    "\n",
    "This notebook takes you through the steps of how to preprocess a high S/N and low S/N test set\n",
    "* required packages: numpy, h5py, vos\n",
    "* required data files: apStar_combined_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import vos\n",
    "\n",
    "datadir='/home/ubuntu/starnet_data/'  # or \"/path/to/my/starnet/directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If you have not downloaded apStar_combined_main.h5 uncomment the below code to copy the file **\n",
    "\n",
    "Note: This file requires 10.3GB. It is necessary to download this file to run  particular notebook, although this notebook can be skipped by downloading the files created here seperately. See $1\\_Download\\_Data.ipynb$ for instructions on how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef starnet_download_file(filename):\\n    vclient = vos.Client()\\n    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\\n    print(filename+' downloaded')\\n\\nstarnet_download_file('apStar_combined_main.h5')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def starnet_download_file(filename):\n",
    "    vclient = vos.Client()\n",
    "    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\n",
    "    print(filename+' downloaded')\n",
    "\n",
    "starnet_download_file('apStar_combined_main.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0_FE',\n",
       " u'0_FE_ERR',\n",
       " u'ALPHA_M',\n",
       " u'AL_FE',\n",
       " u'AL_FE_ERR',\n",
       " u'CA_FE',\n",
       " u'CA_FE_ERR',\n",
       " u'C_FE',\n",
       " u'C_FE_ERR',\n",
       " u'FE_H',\n",
       " u'FE_H_ERR',\n",
       " u'IDs',\n",
       " u'K_FE',\n",
       " u'K_FE_ERR',\n",
       " u'LOGG',\n",
       " u'LOGG_ERR',\n",
       " u'MG_FE',\n",
       " u'MG_FE_ERR',\n",
       " u'MN_FE',\n",
       " u'MN_FE_ERR',\n",
       " u'Main Sequence Flag',\n",
       " u'NA_FE',\n",
       " u'NA_FE_ERR',\n",
       " u'NI_FE',\n",
       " u'NI_FE_ERR',\n",
       " u'N_FE',\n",
       " u'N_FE_ERR',\n",
       " u'PARAM',\n",
       " u'SI_FE',\n",
       " u'SI_FE_ERR',\n",
       " u'S_FE',\n",
       " u'S_FE_ERR',\n",
       " u'TEFF',\n",
       " u'TEFF_ERR',\n",
       " u'TI_FE',\n",
       " u'TI_FE_ERR',\n",
       " u'VRAD',\n",
       " u'VRAD_ERR',\n",
       " u'VSCATTER',\n",
       " u'V_FE',\n",
       " u'V_FE_ERR',\n",
       " u'aspcap_flag',\n",
       " u'error_spectrum',\n",
       " u'num_visits',\n",
       " u'spectrum',\n",
       " u'stacked_snr',\n",
       " u'star_flag',\n",
       " u'targ1_flag',\n",
       " u'targ2_flag']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = datadir + 'apStar_combined_main.h5'\n",
    "F = h5py.File(filename,'r')\n",
    "print('Dataset keys in file: \\n')\n",
    "F.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data into memory**\n",
    "\n",
    "For the testing of StarNet, it is necessary to obtain the spectra, error spectra, combined S/N, and labels, but we need to make eliminations to the test set to obtain the labels of highest validity to compare with, so we will first include the APOGEE_IDs, the S/N of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$, [Fe/H], $V_{scatter}$, STARFLAGs, and ASPCAPFLAGs to make certain eliminations. Once the stars for the test sets have been collected we will then gather the spectra and error spectra and save the two test sets to an h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtainined data for 143482 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = F['IDs'][:,0]\n",
    "combined_snr = F['stacked_snr'][:]\n",
    "starflag = F['star_flag']\n",
    "aspcapflag = F['aspcap_flag']\n",
    "teff = F['TEFF'][:]\n",
    "logg = F['LOGG'][:]\n",
    "fe_h = F['FE_H'][:]\n",
    "vscatter = F['VSCATTER']\n",
    "\n",
    "print('Obtainined data for '+str(len(list(set(list(ap_id)))))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect label normalization data**\n",
    "\n",
    "Create a file that contains the mean and standard deviation for $T_{\\mathrm{eff}}$, $\\log(g)$, and  $[Fe/H]$ in order to normalize labels during training and testing. Ignore values equal to -9999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_and_std.npy saved\n"
     ]
    }
   ],
   "source": [
    "mean = np.array([np.mean(teff[teff!=-9999.]),np.mean(logg[logg!=-9999.]),np.mean(fe_h[fe_h!=-9999.])])\n",
    "std = np.array([np.std(teff[teff!=-9999.]),np.std(logg[logg!=-9999.]),np.std(fe_h[fe_h!=-9999.])])\n",
    "mean_and_std = np.row_stack((mean,std))\n",
    "np.save(datadir+'mean_and_std', mean_and_std)\n",
    "\n",
    "print('mean_and_std.npy saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate out a dataset with good labels**. \n",
    "- STARFLAGs = 0\n",
    "- ASPCAPFLAGs = 0\n",
    "- 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "- -3.0 < [Fe/H]\n",
    "- $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "- $V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35511 stars remain.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "ap_id = ap_id[indices]\n",
    "teff = teff[indices]\n",
    "logg = logg[indices]\n",
    "fe_h = fe_h[indices]\n",
    "combined_snr = combined_snr[indices]\n",
    "\n",
    "print(str(len(list(set(list(ap_id)))))+' stars remain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load test set APOGEE IDs**\n",
    "\n",
    "Load previously created file that contains the training data. We do not want to include any of the APOGEE IDs used in the training set in our test set. This file was created in 2_Preprocessing_of_Training_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = 'training_data.h5'\n",
    "\n",
    "with h5py.File(datadir + savename, \"r\") as f:\n",
    "    \n",
    "    train_ap_id = f['Ap_ID'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate data for High S/N test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set includes 21033 combined spectra\n"
     ]
    }
   ],
   "source": [
    "indices_test = [i for i, item in enumerate(ap_id) if item not in train_ap_id]\n",
    "\n",
    "test_ap_id = ap_id[indices_test]\n",
    "test_teff = teff[indices_test]\n",
    "test_logg = logg[indices_test]\n",
    "test_fe_h = fe_h[indices_test]\n",
    "test_combined_snr = combined_snr[indices_test]\n",
    "\n",
    "indices_test_set = indices[indices_test] # These indices will be used to index through the spectra\n",
    "\n",
    "print('Test set includes '+str(len(test_ap_id))+' combined spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now collect spectra and error spectra. Then normalize each spectrum and save the data**\n",
    "\n",
    "**Steps taken to normalize spectra:**\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values\n",
    "4. Error spectra must also be normalized with the same median values for use in the error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.h5 has been saved as the low S/N test set to be used in 5_Test_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'test_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(path) \n",
    "os.remove(datadir + savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "\n",
    "with h5py.File(datadir + savename, \"a\") as f:\n",
    "    \n",
    "    # Create datasets for your test data file \n",
    "    spectra_ds = f.create_dataset('spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    error_spectra_ds = f.create_dataset('error_spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    teff_ds = f.create_dataset('TEFF', test_teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', test_logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', test_fe_h.shape, dtype=\"f\")\n",
    "    combined_snr_ds = f.create_dataset('combined_snr', test_combined_snr.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_ID', test_ap_id.shape, dtype=\"S18\")\n",
    "    \n",
    "    # Save data to data file\n",
    "    teff_ds[:] = test_teff\n",
    "    logg_ds[:] = test_logg\n",
    "    fe_h_ds[:] = test_fe_h\n",
    "    combined_snr_ds[:] = test_combined_snr\n",
    "    ap_id_ds[:] = test_ap_id.tolist()\n",
    "        \n",
    "    # Collect spectra\n",
    "    first_entry=True\n",
    "    \n",
    "    for i in indices_test_set:\n",
    "\n",
    "        spectrum = F['spectrum'][i:i+1]\n",
    "        err_spectrum = F['error_spectrum'][i:i+1]\n",
    "\n",
    "\n",
    "        # NORMALIZE SPECTRUM\n",
    "        # Separate spectra into chips\n",
    "        blue_sp = spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "        \n",
    "        blue_sp_med = np.median(blue_sp, axis=1)\n",
    "        green_sp_med = np.median(green_sp, axis=1)\n",
    "        red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "        #Normalize spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine spectra\n",
    "        spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        # Normalize error spectrum using the same method\n",
    "        # Separate error spectra into chips\n",
    "\n",
    "        blue_sp = err_spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = err_spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = err_spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "\n",
    "        # Normalize error spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine error spectra\n",
    "        err_spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        \n",
    "        if first_entry:\n",
    "            spectra_ds[0] = spectrum\n",
    "            error_spectra_ds[0] = err_spectrum\n",
    "            \n",
    "            first_entry=False\n",
    "        else:\n",
    "            spectra_ds.resize(spectra_ds.shape[0]+1, axis=0)\n",
    "            error_spectra_ds.resize(error_spectra_ds.shape[0]+1, axis=0)\n",
    "\n",
    "            spectra_ds[-1] = spectrum\n",
    "            error_spectra_ds[-1] = err_spectrum\n",
    "\n",
    "print(savename+' has been saved as the low S/N test set to be used in 5_Test_Model.ipynb')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
