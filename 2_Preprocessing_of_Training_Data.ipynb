{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training and test set for StarNet\n",
    "This notebook takes you through the steps of how to pre-process the training data necessary for training StarNet and separate out a high S/N test set.\n",
    "\n",
    "Requirements:\n",
    "- python packages: `numpy h5py vos`\n",
    "* required data files: apStar_visits_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import vos\n",
    "\n",
    "datadir='/home/ubuntu/starnet_data/'  # or \"/path/to/my/starnet/directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If you have not downloaded apStar_visits_main.h5 uncomment the below code to copy the file **\n",
    "\n",
    "Note: This file requires 38.6GB. It is necessary to download this file to run this particular notebook, although this notebook can be skipped by downloading the files created here seperately. See $1\\_Download\\_Data.ipynb$ for instructions on how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef starnet_download_file(filename):\\n    vclient = vos.Client()\\n    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\\n    print(filename+' downloaded')\\n    \\nstarnet_download_file('apStar_visits_main.h5')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def starnet_download_file(filename):\n",
    "    vclient = vos.Client()\n",
    "    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\n",
    "    print(filename+' downloaded')\n",
    "    \n",
    "starnet_download_file('apStar_visits_main.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the file that contains individual visit spectra along with APOGEE data associated with each star**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'0_FE',\n",
       " u'0_FE_ERR',\n",
       " u'ALPHA_M',\n",
       " u'AL_FE',\n",
       " u'AL_FE_ERR',\n",
       " u'CA_FE',\n",
       " u'CA_FE_ERR',\n",
       " u'C_FE',\n",
       " u'C_FE_ERR',\n",
       " u'FE_H',\n",
       " u'FE_H_ERR',\n",
       " u'IDs',\n",
       " u'K_FE',\n",
       " u'K_FE_ERR',\n",
       " u'LOGG',\n",
       " u'LOGG_ERR',\n",
       " u'MG_FE',\n",
       " u'MG_FE_ERR',\n",
       " u'MN_FE',\n",
       " u'MN_FE_ERR',\n",
       " u'NA_FE',\n",
       " u'NA_FE_ERR',\n",
       " u'NI_FE',\n",
       " u'NI_FE_ERR',\n",
       " u'N_FE',\n",
       " u'N_FE_ERR',\n",
       " u'PARAM',\n",
       " u'SI_FE',\n",
       " u'SI_FE_ERR',\n",
       " u'S_FE',\n",
       " u'S_FE_ERR',\n",
       " u'TEFF',\n",
       " u'TEFF_ERR',\n",
       " u'TI_FE',\n",
       " u'TI_FE_ERR',\n",
       " u'VRAD',\n",
       " u'VRAD_ERR',\n",
       " u'VSCATTER',\n",
       " u'V_FE',\n",
       " u'V_FE_ERR',\n",
       " u'aspcap_flag',\n",
       " u'bluegreen_persist',\n",
       " u'error_spectrum',\n",
       " u'greenred_persist',\n",
       " u'num_visits',\n",
       " u'spectrum',\n",
       " u'stacked_snr',\n",
       " u'star_flag',\n",
       " u'star_flag_indiv',\n",
       " u'targ1_flag',\n",
       " u'targ2_flag',\n",
       " u'visit_snr']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = datadir + 'apStar_visits_main_dr13.h5'\n",
    "\n",
    "F = h5py.File(filename,'r')\n",
    "print('Dataset keys in file: \\n')\n",
    "F.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load the APOGEE data set into memory**\n",
    "\n",
    "For the training of StarNet, it is only necessary to obtain the spectra and labels, but we need to set restrictions on the training set to obtain the labels of highest validity so we will first include APOGEE_IDs, the spectra, the S/N of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$,  [Fe/H],  $V_{scatter}$,  STARFLAGs, and ASPCAPFLAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained data for 559484 individual visits from 143467 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = F['IDs'][:,0]\n",
    "combined_snr = F['stacked_snr']\n",
    "starflag = F['star_flag']\n",
    "aspcapflag = F['aspcap_flag']\n",
    "teff = F['TEFF'][:]\n",
    "logg = F['LOGG'][:]\n",
    "fe_h = F['FE_H'][:]\n",
    "vscatter = F['VSCATTER']\n",
    "\n",
    "print('Obtained data for '+str(len(ap_id))+' individual visits from '+str(len(list(set(list(ap_id)))))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate out a dataset with good labels**\n",
    "- combined spectral S/N $\\geq$ 200\n",
    "- STARFLAG = 0\n",
    "- ASPCAPFLAG = 0\n",
    "- 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "- -3.0 dex < [Fe/H]\n",
    "- $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "- $V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snr_min = 200.\n",
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53135 visits remain.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(combined_snr[:]>=snr_min)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "print(str(len(indices))+' visits remain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the first **$num\\_ref$** visits for the reference set**\n",
    "\n",
    "We shuffle around the data to avoid local effects.\n",
    "Later on, it will be be split into training and cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference set includes 44784 individual visits from 14498 stars.\n"
     ]
    }
   ],
   "source": [
    "num_ref = 44784 # number of reference spectra\n",
    "\n",
    "indices_ref = indices[0:num_ref]\n",
    "np.random.shuffle(indices_ref)\n",
    "\n",
    "ap_id_ref = ap_id[indices_ref]\n",
    "teff = teff[indices_ref]\n",
    "logg = logg[indices_ref]\n",
    "fe_h = fe_h[indices_ref]\n",
    "\n",
    "print('Reference set includes '+str(len(ap_id_ref))+' individual visits from '+str(len(set(ap_id_ref)))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate a test set of APOGEE IDs**\n",
    "\n",
    "- These APOGEE IDs will be processed in a following notebook to create StarNet's High S/N test set\n",
    "- Make sure there are no duplicates from test set that are also in training set (this is necessary because there are some duplicates in the APOGEE v603.fits file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2651 stars to be processed for the High S/N test set.\n"
     ]
    }
   ],
   "source": [
    "indices_test = indices[num_ref:]\n",
    "\n",
    "ap_id_test = ap_id[indices_test]\n",
    "ap_id_test = list(set(ap_id_test)-set(ap_id_ref))\n",
    "np.save(datadir + 'high_snr_test_apids', ap_id_test)\n",
    "print(str(len(ap_id_test))+' stars to be processed for the High S/N test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now collect individual visit spectra, normalize each spectrum, and save data**\n",
    "\n",
    "**Normalize spectra**\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.h5 has been saved as the reference set to be used in 4_Train_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'training_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(path) \n",
    "#os.remove(datadir + savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "\n",
    "with h5py.File(datadir + savename, \"a\") as f:\n",
    "    \n",
    "    # Create datasets for your reference data file \n",
    "    spectra_ds = f.create_dataset('spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    teff_ds = f.create_dataset('TEFF', teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', fe_h.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_ID', ap_id_ref.shape, dtype=\"S18\")\n",
    "    \n",
    "    teff_ds[:] = teff\n",
    "    logg_ds[:] = logg\n",
    "    fe_h_ds[:] = fe_h\n",
    "    ap_id_ds[:] = ap_id_ref.tolist()\n",
    "        \n",
    "    first_entry=True\n",
    "    \n",
    "    for i in indices_ref:\n",
    "\n",
    "        spectrum = F['spectrum'][i:i+1]\n",
    "\n",
    "\n",
    "        # NORMALIZE SPECTRUM\n",
    "        # Separate spectra into chips\n",
    "        blue_sp = spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "\n",
    "        # Normalize spectra by chips\n",
    "\n",
    "        blue_sp = (blue_sp.T/np.median(blue_sp, axis=1)).T\n",
    "        green_sp = (green_sp.T/np.median(green_sp, axis=1)).T\n",
    "        red_sp = (red_sp.T/np.median(red_sp, axis=1)).T \n",
    "\n",
    "        # Recombine spectra\n",
    "\n",
    "        spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        if first_entry:\n",
    "            spectra_ds[0] = spectrum\n",
    "            first_entry=False\n",
    "        else:\n",
    "            spectra_ds.resize(spectra_ds.shape[0]+1, axis=0)\n",
    "\n",
    "            spectra_ds[-1] = spectrum\n",
    "\n",
    "print(savename+' has been saved as the reference set to be used in 4_Train_Model.ipynb')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
