{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain training data and separate out a High S/N Test set\n",
    "\n",
    "## This notebook takes you through the steps of how to preprocess the training data into the form necessary for training StarNet\n",
    "## required packages:\n",
    "### - numpy\n",
    "### - h5py\n",
    "## required data files:\n",
    "### - apStar_visits_main.h5 (can be downloaded in $1\\_Download\\_Data.ipynb$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load apStar_visits_main.h5, a file that contains individual visit spectra along with APOGEE data associated with each star.  File can be downloaded in $1\\_Download\\_Data.ipynb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'apStar_visits_main.h5'\n",
    "f = h5py.File(filename,\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n",
      "0_H\n",
      "0_H_ERR\n",
      "ALPHA_M\n",
      "AL_H\n",
      "AL_H_ERR\n",
      "CA_H\n",
      "CA_H_ERR\n",
      "C_H\n",
      "C_H_ERR\n",
      "FE_H\n",
      "FE_H_ERR\n",
      "IDs\n",
      "K_H\n",
      "K_H_ERR\n",
      "LOGG\n",
      "LOGG_ERR\n",
      "MG_H\n",
      "MG_H_ERR\n",
      "MN_H\n",
      "MN_H_ERR\n",
      "NA_H\n",
      "NA_H_ERR\n",
      "NI_H\n",
      "NI_H_ERR\n",
      "N_H\n",
      "N_H_ERR\n",
      "SI_H\n",
      "SI_H_ERR\n",
      "S_H\n",
      "S_H_ERR\n",
      "TEFF\n",
      "TEFF_ERR\n",
      "TI_H\n",
      "TI_H_ERR\n",
      "VRAD\n",
      "VRAD_ERR\n",
      "VSCATTER\n",
      "V_H\n",
      "V_H_ERR\n",
      "aspcap_flag\n",
      "bluegreen_persist\n",
      "error_spectrum\n",
      "greenred_persist\n",
      "num_visits\n",
      "spectrum\n",
      "stacked_snr\n",
      "star_flag\n",
      "targ1_flag\n",
      "targ2_flag\n",
      "visit_snr\n"
     ]
    }
   ],
   "source": [
    "print('Dataset keys in file: \\n')\n",
    "for i in f.keys(): print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the training of StarNet, it is only necessary to obtain the spectra and labels, but we need to set restrictions on the training set to obtain the labels of highest validity so we will first include the $APOGEE\\_IDs$, the spectra, the $S/N$ of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$,  $[Fe/H]$,  $V_{scatter}$,  $STARFLAGs$, and $ASPCAPFLAGs$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained spectra and data for 559359 from 142333 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = f['IDs'][:,0]\n",
    "spectra = f['spectrum'][:]\n",
    "combined_snr = f['stacked_snr'][:]\n",
    "starflag = f['star_flag'][:]\n",
    "aspcapflag = f['aspcap_flag'][:]\n",
    "teff = f['TEFF'][:]\n",
    "logg = f['LOGG'][:]\n",
    "fe_h = f['FE_H'][:]\n",
    "vscatter = f['VSCATTER'][:]\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Obtained spectra and data for '+str(len(ap_id))+' from '+str(len(list(set(list(ap_id)))))+' stars.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate out a dataset with good labels:\n",
    "## Default restrictions:\n",
    "### - combined spectral S/N $\\geq$ 200\n",
    "### - STARFLAG = 0\n",
    "### - ASPCAPFLAG = 0\n",
    "### - 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "### - -3.0 dex < $[Fe/H]$\n",
    "### - $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "### -$V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snr_min = 200.\n",
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64076 spectra remain from 22773 stars.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(combined_snr[:]>=snr_min)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "ap_id = ap_id[indices]\n",
    "spectra = spectra[indices]\n",
    "teff = teff[indices]\n",
    "logg = logg[indices]\n",
    "fe_h = fe_h[indices]\n",
    "\n",
    "print(str(len(ap_id))+' spectra remain from '+str(len(list(set(list(ap_id)))))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the first $num\\_train$ visits for the reference set (later to be split into training and cross-validation sets)\n",
    "### default:\n",
    "#### - num_train = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference set includes 50000 individual visit spectra from 17841 stars\n"
     ]
    }
   ],
   "source": [
    "num_train = 50000\n",
    "\n",
    "ap_id_train = ap_id[0:num_train]\n",
    "spectra = spectra[0:num_train]\n",
    "teff = teff[0:num_train]\n",
    "logg = logg[0:num_train]\n",
    "fe_h = fe_h[0:num_train]\n",
    "\n",
    "print('Reference set includes '+str(len(ap_id_train))+' individual visit spectra from '+str(len(set(ap_id_train)))+' stars.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate a test set of $APOGEE\\_IDs$ to be processed in $3\\_Preprocessing\\_of\\_Test\\_Data.ipynb$ as StarNet's $High\\_S/N\\_Test\\_Set$\n",
    "## make sure there are no duplicates from test set that are also in training set (this is necessary because there are some duplicates in the APOGEE v603.fits file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4932 stars to be processed for the High S/N test set\n"
     ]
    }
   ],
   "source": [
    "ap_id_test = ap_id[50000:]\n",
    "\n",
    "ap_id_test = list(set(ap_id_test)-set(ap_id_train))\n",
    "print(str(len(ap_id_test))+' stars to be processed for the High S/N test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save APOGEE IDs for High S/N Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOGEE IDs for the high S/N test set are saved to be used in 2_Preprocessing_of_Test_Data.ipynb\n"
     ]
    }
   ],
   "source": [
    "np.save('high_snr_test_apids', ap_id_test)\n",
    "print('APOGEE IDs for the high S/N test set are saved to be used in 3_Preprocessing_of_Test_Data.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize spectra:\n",
    "### 1. separate into three chips\n",
    "### 2. divide by median value in each chip\n",
    "### 3. recombine each spectrum into a vector of 7214 flux values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference spectra dataset now contains 50000 spectra, each with 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# Separate spectra into chips\n",
    "\n",
    "blue_sp = spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "# Normalize spectra by chips\n",
    "\n",
    "blue_sp = (blue_sp.T/np.median(blue_sp, axis=1)).T\n",
    "green_sp = (green_sp.T/np.median(green_sp, axis=1)).T\n",
    "red_sp = (red_sp.T/np.median(red_sp, axis=1)).T \n",
    "\n",
    "# Recombine spectra\n",
    "\n",
    "spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "\n",
    "print('Reference spectra dataset now contains ' + str(spectra.shape[0])+' spectra, each with '+str(spectra.shape[1])+' wavelength bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save new training data file with APOGEE IDs, spectra, and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.h5 has been saved as the reference set to be used in 3_Train_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'training_data.h5'\n",
    "# if path already exist, you must remove it first using os.remove(path) \n",
    "#os.remove(savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "with h5py.File(savename, \"a\") as f:\n",
    "     \n",
    "    spectra_ds = f.create_dataset('spectra', spectra.shape, dtype=\"f\")\n",
    "    teff_ds = f.create_dataset('TEFF', teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', fe_h.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_IDs', ap_id_train.shape, dtype=\"S18\")\n",
    "    \n",
    "    spectra_ds[:] = spectra\n",
    "    teff_ds[:] = teff\n",
    "    logg_ds[:] = logg\n",
    "    fe_h_ds[:] = fe_h\n",
    "    ap_id_ds[:] = ap_id_train.tolist()\n",
    "\n",
    "print(savename+' has been saved as the reference set to be used in 4_Train_Model.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
