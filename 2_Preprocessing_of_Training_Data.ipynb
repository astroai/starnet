{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training and test set for StarNet\n",
    "This notebook takes you through the steps of how to pre-process the training data necessary for training StarNet and separate out a high S/N test set.\n",
    "\n",
    "Requirements:\n",
    "- python packages: `numpy h5py vos`\n",
    "* required data files: apStar_visits_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import vos\n",
    "\n",
    "datadir=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If you have not downloaded apStar_visits_main.h5 uncomment the below code to copy the file **\n",
    "\n",
    "Note: This file requires 38.6GB. It is necessary to download this file to run this particular notebook, although this notebook can be skipped by downloading the files created here seperately. See $1\\_Download\\_Data.ipynb$ for instructions on how to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef starnet_download_file(filename):\\n    vclient = vos.Client()\\n    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\\n    print(filename+' downloaded')\\n    \\nstarnet_download_file('apStar_visits_main.h5')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def starnet_download_file(filename):\n",
    "    vclient = vos.Client()\n",
    "    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\n",
    "    print(filename+' downloaded')\n",
    "    \n",
    "starnet_download_file('apStar_visits_main.h5')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the file that contains individual visit spectra along with APOGEE data associated with each star**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n",
      "['0_FE', '0_FE_ERR', 'ALPHA_M', 'AL_FE', 'AL_FE_ERR', 'CA_FE', 'CA_FE_ERR', 'C_FE', 'C_FE_ERR', 'FE_H', 'FE_H_ERR', 'IDs', 'K_FE', 'K_FE_ERR', 'LOGG', 'LOGG_ERR', 'MG_FE', 'MG_FE_ERR', 'MN_FE', 'MN_FE_ERR', 'NA_FE', 'NA_FE_ERR', 'NI_FE', 'NI_FE_ERR', 'N_FE', 'N_FE_ERR', 'PARAM', 'SI_FE', 'SI_FE_ERR', 'S_FE', 'S_FE_ERR', 'TEFF', 'TEFF_ERR', 'TI_FE', 'TI_FE_ERR', 'VRAD', 'VRAD_ERR', 'VSCATTER', 'V_FE', 'V_FE_ERR', 'aspcap_flag', 'bluegreen_persist', 'error_spectrum', 'greenred_persist', 'num_visits', 'spectrum', 'stacked_snr', 'star_flag', 'star_flag_indiv', 'targ1_flag', 'targ2_flag', 'visit_snr']\n"
     ]
    }
   ],
   "source": [
    "filename = datadir + 'apStar_visits_main.h5'\n",
    "\n",
    "F = h5py.File(filename,'r')\n",
    "print('Dataset keys in file: \\n')\n",
    "print(list(F.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load the APOGEE data set into memory**\n",
    "\n",
    "For the training of StarNet, it is only necessary to obtain the spectra and labels, but we need to set restrictions on the training set to obtain the labels of highest validity so we will first include APOGEE_IDs, the spectra, the S/N of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$,  [Fe/H],  $V_{scatter}$,  STARFLAGs, and ASPCAPFLAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained data for 559484 individual visits from 143467 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = F['IDs'][:,0]\n",
    "combined_snr = F['stacked_snr']\n",
    "starflag = F['star_flag']\n",
    "aspcapflag = F['aspcap_flag']\n",
    "teff = F['TEFF'][:]\n",
    "logg = F['LOGG'][:]\n",
    "fe_h = F['FE_H'][:]\n",
    "vscatter = F['VSCATTER']\n",
    "\n",
    "print('Obtained data for '+str(len(ap_id))+' individual visits from '+str(len(list(set(list(ap_id)))))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate out a dataset with good labels**\n",
    "- combined spectral S/N $\\geq$ 200\n",
    "- STARFLAG = 0\n",
    "- ASPCAPFLAG = 0\n",
    "- 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "- -3.0 dex < [Fe/H]\n",
    "- $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "- $V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snr_min = 200.\n",
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53135 individual visits from 17149 stars remain.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(combined_snr[:]>=snr_min)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "ap_id_high_snr = ap_id[indices]\n",
    "print(str(len(ap_id_high_snr))+' individual visits from '+str(len(set(ap_id_high_snr)))+' stars remain.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the first **$num\\_ref$** visits for the reference set**\n",
    "\n",
    "We shuffle around the data to avoid local effects.\n",
    "Later on, it will be be split into training and cross-validation sets.\n",
    "The remaining high S/N spectra will be used in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference set includes 44784 individual visits from 14498 stars.\n"
     ]
    }
   ],
   "source": [
    "num_ref = 44784 # number of reference spectra\n",
    "\n",
    "indices_ref = indices[0:num_ref]\n",
    "np.random.shuffle(indices_ref)\n",
    "\n",
    "ap_id_ref = ap_id[indices_ref]\n",
    "teff = teff[indices_ref]\n",
    "logg = logg[indices_ref]\n",
    "fe_h = fe_h[indices_ref]\n",
    "\n",
    "print('Reference set includes '+str(len(ap_id_ref))+' individual visits from '+str(len(set(ap_id_ref)))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now collect individual visit spectra, normalize each spectrum, and save data**\n",
    "\n",
    "**Normalize spectra**\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.h5 has been saved as the reference set to be used in 4_Train_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'training_data.h5'\n",
    "\n",
    "with h5py.File(datadir + savename, \"w\") as f:\n",
    "    \n",
    "    # Create datasets for your reference data file \n",
    "    spectra_ds = f.create_dataset('spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    teff_ds = f.create_dataset('TEFF', teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', fe_h.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_ID', ap_id_ref.shape, dtype=\"S18\")\n",
    "    \n",
    "    teff_ds[:] = teff\n",
    "    logg_ds[:] = logg\n",
    "    fe_h_ds[:] = fe_h\n",
    "    ap_id_ds[:] = ap_id_ref.tolist()\n",
    "        \n",
    "    first_entry=True\n",
    "    \n",
    "    for i in indices_ref:\n",
    "\n",
    "        spectrum = F['spectrum'][i:i+1]\n",
    "        spectrum[np.isnan(spectrum)]=0.\n",
    "\n",
    "        # NORMALIZE SPECTRUM\n",
    "        # Separate spectra into chips\n",
    "        blue_sp = spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "\n",
    "        # Normalize spectra by chips\n",
    "\n",
    "        blue_sp = (blue_sp.T/np.median(blue_sp, axis=1)).T\n",
    "        green_sp = (green_sp.T/np.median(green_sp, axis=1)).T\n",
    "        red_sp = (red_sp.T/np.median(red_sp, axis=1)).T \n",
    "\n",
    "        # Recombine spectra\n",
    "\n",
    "        spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        if first_entry:\n",
    "            spectra_ds[0] = spectrum\n",
    "            first_entry=False\n",
    "        else:\n",
    "            spectra_ds.resize(spectra_ds.shape[0]+1, axis=0)\n",
    "\n",
    "            spectra_ds[-1] = spectrum\n",
    "\n",
    "print(savename+' has been saved as the reference set to be used in 4_Train_Model.ipynb')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
