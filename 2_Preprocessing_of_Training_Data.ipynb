{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training and test set for StarNet\n",
    "This notebook takes you through the steps of how to pre-process the training data necessary for training StarNet and separate out a high S/N test set.\n",
    "\n",
    "Requirements:\n",
    "- python packages: `numpy h5py vos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apStar_visits_main.h5 downloaded\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import vos\n",
    "\n",
    "datadir='/home/ubuntu/starnet_data/'  # or \"/path/to/my/starnet/directory\"\n",
    "\n",
    "def starnet_download_file(filename):\n",
    "    vclient = vos.Client()\n",
    "    vclient.copy('vos:starnet/public/'+filename, datadir+filename)\n",
    "    print(filename+' downloaded')\n",
    "    \n",
    "starnet_download_file('apStar_visits_main.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the file that contains individual visit spectra along with APOGEE data associated with each star**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = datadir + 'apStar_visits_main.h5'\n",
    "f = h5py.File(filename,'r')\n",
    "print('Dataset keys in file: \\n')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load the APOGEE data set into memory**\n",
    "\n",
    "For the training of StarNet, it is only necessary to obtain the spectra and labels, but we need to set restrictions on the training set to obtain the labels of highest validity so we will first include APOGEE_IDs, the spectra, the S/N of the combined spectra, $T_{\\mathrm{eff}}$, $\\log(g)$,  [Fe/H],  $V_{scatter}$,  STARFLAGs, and ASPCAPFLAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained spectra and data for 559359 from 142333 stars.\n"
     ]
    }
   ],
   "source": [
    "ap_id = f['IDs'][:,0]\n",
    "spectra = f['spectrum'][:]\n",
    "combined_snr = f['stacked_snr'][:]\n",
    "starflag = f['star_flag'][:]\n",
    "aspcapflag = f['aspcap_flag'][:]\n",
    "teff = f['TEFF'][:]\n",
    "logg = f['LOGG'][:]\n",
    "fe_h = f['FE_H'][:]\n",
    "vscatter = f['VSCATTER'][:]\n",
    "\n",
    "f.close()\n",
    "\n",
    "print('Obtained spectra and data for '+str(len(ap_id))+' from '+str(len(list(set(list(ap_id)))))+' stars.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate out a dataset with good labels**\n",
    "- combined spectral S/N $\\geq$ 200\n",
    "- STARFLAG = 0\n",
    "- ASPCAPFLAG = 0\n",
    "- 4000K < $T_{\\mathrm{eff}}$ < 5500K\n",
    "- -3.0 dex < [Fe/H]\n",
    "- $\\log(g)$ $\\neq$ -9999. (value defined by ASPCAP when no ASPCAP labels are given)\n",
    "- $V_{scatter}$ < 1.0 km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snr_min = 200.\n",
    "teff_min = 4000.\n",
    "teff_max = 5500.\n",
    "vscatter_max = 1.\n",
    "fe_h_min = -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53548 spectra remain from 17001 stars.\n"
     ]
    }
   ],
   "source": [
    "indices, cols = np.where((aspcapflag[:]==0.)&(starflag[:]==0.)&(combined_snr[:]>=snr_min)&(vscatter[:]<vscatter_max)&(fe_h[:]>fe_h_min)&(teff[:]>teff_min)&(teff[:]<teff_max)&(logg[:]!=-9999.).reshape(len(ap_id),1))\n",
    "\n",
    "ap_id = ap_id[indices]\n",
    "spectra = spectra[indices]\n",
    "teff = teff[indices]\n",
    "logg = logg[indices]\n",
    "fe_h = fe_h[indices]\n",
    "\n",
    "print(str(len(ap_id))+' spectra remain from '+str(len(list(set(list(ap_id)))))+' stars.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the  visits for the reference set**\n",
    "\n",
    "Later on, it will be be split into training and cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference set includes 44784 individual visit spectra from 14221 stars.\n"
     ]
    }
   ],
   "source": [
    "num_ref = 44784 # number of reference spectra\n",
    "\n",
    "ap_id_train = ap_id[0:num_ref]\n",
    "spectra = spectra[0:num_ref]\n",
    "teff = teff[0:num_ref]\n",
    "logg = logg[0:num_ref]\n",
    "fe_h = fe_h[0:num_ref]\n",
    "\n",
    "print('Reference set includes '+str(len(ap_id_train))+' individual visit spectra from '+str(len(set(ap_id_train)))+' stars.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate a test set of APOGEE IDs**\n",
    "\n",
    "- These APOGEE IDs will be processed in a following notebook to create StarNet's High S/N test set\n",
    "- Make sure there are no duplicates from test set that are also in training set (this is necessary because there are some duplicates in the APOGEE v603.fits file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2780 stars to be processed for the High S/N test set.\n"
     ]
    }
   ],
   "source": [
    "ap_id_test = ap_id[num_ref:]\n",
    "ap_id_test = list(set(ap_id_test)-set(ap_id_train))\n",
    "np.save('high_snr_test_apids', ap_id_test)\n",
    "print(str(len(ap_id_test))+' stars to be processed for the High S/N test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize spectra**\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference spectra dataset now contains 44784 spectra, each with 7214 wavelength bins\n"
     ]
    }
   ],
   "source": [
    "# Separate spectra into chips\n",
    "\n",
    "blue_sp = spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "# Normalize spectra by chips\n",
    "\n",
    "blue_sp = (blue_sp.T / np.median(blue_sp, axis=1)).T\n",
    "green_sp = (green_sp.T / np.median(green_sp, axis=1)).T\n",
    "red_sp = (red_sp.T / np.median(red_sp, axis=1)).T \n",
    "\n",
    "# Recombine spectra\n",
    "\n",
    "spectra = np.column_stack((blue_sp, green_sp, red_sp))\n",
    "\n",
    "print('Reference spectra dataset now contains ' + str(spectra.shape[0])+' spectra, each with '+str(spectra.shape[1])+' wavelength bins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save new training data file**\n",
    "\n",
    "with APOGEE IDs, spectra, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.h5 has been saved as the reference set to be used in 4_Train_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = datadir + 'training_data.h5'\n",
    "os.remove(savename)\n",
    "dt = h5py.special_dtype(vlen=bytes)\n",
    "with h5py.File(savename, 'a') as f:\n",
    "     \n",
    "    spectra_ds = f.create_dataset('spectra', spectra.shape, dtype=\"f\")\n",
    "    teff_ds = f.create_dataset('TEFF', teff.shape, dtype=\"f\")\n",
    "    logg_ds = f.create_dataset('LOGG', logg.shape, dtype=\"f\")\n",
    "    fe_h_ds = f.create_dataset('FE_H', fe_h.shape, dtype=\"f\")\n",
    "    ap_id_ds = f.create_dataset('Ap_IDs', ap_id_train.shape, dtype=\"S18\")\n",
    "    \n",
    "    spectra_ds[:] = spectra\n",
    "    teff_ds[:] = teff\n",
    "    logg_ds[:] = logg\n",
    "    fe_h_ds[:] = fe_h\n",
    "    ap_id_ds[:] = ap_id_train.tolist()\n",
    "\n",
    "print(savename + ' has been saved as the reference set to be used in 4_Train_Model.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
