{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propogate Errors\n",
    "\n",
    "This notebook takes you through the steps of how to propogate errors for through the neural network model\n",
    "\n",
    "* required packages: `numpy h5py keras`\n",
    "* data files: \n",
    "    - starnet_cnn.h5\n",
    "    - mean_and_std.npy\n",
    "    - test_data.h5\n",
    "    - apStar_combined_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "datadir= \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define path variables for your keras model, denormalization data, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = datadir + 'starnet_cnn.h5'\n",
    "denormalization_path = datadir + 'mean_and_std.npy'\n",
    "test_data_path = datadir + 'test_data.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define functions to:**\n",
    "\n",
    "1. convert the keras model into a tensorflow graph\n",
    "2. load a tensorflow graph\n",
    "3. compute the jacobian matrix\n",
    "4. compute the covariance\n",
    "5. compute the variance\n",
    "\n",
    "Note: these functions can be combined into one, but they are separated here to allow users to extract intermediate results for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keras_to_tf(weight_file,input_fld='',output_fld=''):\n",
    "    \n",
    "    import os\n",
    "    import os.path as osp\n",
    "    from tensorflow.python.framework import graph_util\n",
    "    from tensorflow.python.framework import graph_io\n",
    "    from keras.models import load_model\n",
    "    from keras import backend as K\n",
    "    \n",
    "    \n",
    "    # weight_file is a .h5 keras model file\n",
    "    output_node_names_of_input_network = [\"pred0\"] \n",
    "    output_node_names_of_final_network = 'output_node'\n",
    "    output_graph_name = weight_file[:-2]+'pb'\n",
    "    weight_file_path = osp.join(input_fld, weight_file)\n",
    "    \n",
    "    net_model = load_model(weight_file_path)\n",
    "\n",
    "    num_output = len(output_node_names_of_input_network)\n",
    "    pred = [None]*num_output\n",
    "    pred_node_names = [None]*num_output\n",
    "    for i in range(num_output):\n",
    "        pred_node_names[i] = output_node_names_of_final_network+str(i)\n",
    "        pred[i] = tf.identity(net_model.output[i], name=pred_node_names[i])\n",
    "        \n",
    "    sess = K.get_session()\n",
    "    \n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)\n",
    "    graph_io.write_graph(constant_graph, output_fld, output_graph_name, as_text=False)\n",
    "    print('saved the constant graph (ready for inference) at: ', osp.join(output_fld, output_graph_name))\n",
    "    \n",
    "    return output_fld+output_graph_name\n",
    "\n",
    "def load_graph(frozen_graph_filename):\n",
    "\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(\n",
    "            graph_def, \n",
    "            input_map=None, \n",
    "            return_elements=None, \n",
    "            name=\"prefix\", \n",
    "            op_dict=None, \n",
    "            producer_op_list=None\n",
    "        )\n",
    "        \n",
    "    input_name = graph.get_operations()[0].name+':0'\n",
    "    output_name = graph.get_operations()[-1].name+':0'\n",
    "    \n",
    "    return graph, input_name, output_name\n",
    "\n",
    "def compute_jacobian_from_tf_model_path(tf_model_path,input_data,denormalize=None):\n",
    "\n",
    "    tf_model,tf_input,tf_output = load_graph(tf_model_path)    \n",
    "    \n",
    "    x = tf_model.get_tensor_by_name(tf_input)\n",
    "    \n",
    "    if denormalize==None:\n",
    "        y = tf_model.get_tensor_by_name(tf_output)\n",
    "    else:\n",
    "        y = denormalize(tf_model.get_tensor_by_name(tf_output))\n",
    "        \n",
    "    y_list = tf.unstack(y)\n",
    "    num_outputs = y.shape.as_list()[0]\n",
    "    \n",
    "    if input_data.shape[0]==1:\n",
    "        with tf.Session(graph=tf_model) as sess:\n",
    "            y_out = sess.run([tf.gradients(y_, x)[0] for y_ in y_list], feed_dict={\n",
    "                x: input_data\n",
    "            })\n",
    "            jacobian = np.asarray(y_out)\n",
    "            jacobian = jacobian[:,:,:,0]\n",
    "    else:\n",
    "        print('\\nCreating jacobian matrices for '+str(len(input_data))+' spectra...\\n')\n",
    "        print_count = int(len(input_data)/10)\n",
    "        if print_count==0:\n",
    "            print_count=1     \n",
    "            \n",
    "        jacobian = np.zeros((num_outputs,input_data.shape[0],input_data.shape[1]))\n",
    "        for i in range(input_data.shape[0]):\n",
    "            with tf.Session(graph=tf_model) as sess:\n",
    "                y_out = sess.run([tf.gradients(y_, x)[0] for y_ in y_list], feed_dict={\n",
    "                    x: input_data[i:i+1]\n",
    "                })\n",
    "            jac_temp = np.asarray(y_out)\n",
    "            jacobian[:,i:i+1,:]=jac_temp[:,:,:,0]\n",
    "            if (i+1)%print_count==0:\n",
    "                print(str(i+1)+' jacobians completed...\\n')\n",
    "        print('All '+str(i+1)+' jacobians completed.\\n')\n",
    "    return jacobian\n",
    "\n",
    "def compute_covariance_from_tf_model_path(tf_model_path,input_data,var,denormalize=None):\n",
    "    jac_matrix = compute_jacobian_from_tf_model_path(tf_model_path,input_data,denormalize)\n",
    "    var[var > 6] = 0\n",
    "    jac_matrix = np.nan_to_num(jac_matrix)\n",
    "    covariance = np.einsum('ijk,kjl->jil',(jac_matrix*(var**2)),jac_matrix.T)\n",
    "    return covariance\n",
    "\n",
    "def compute_variance_from_tf_model_path(tf_model_path,input_data,var,denormalize=None):\n",
    "    covariance = compute_covariance_from_tf_model_path(tf_model_path,input_data,var,denormalize)\n",
    "    return np.diagonal(covariance, offset=0, axis1=1, axis2=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model as a tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 10 variables.\n",
      "Converted 10 variables to const ops.\n",
      "('saved the constant graph (ready for inference) at: ', '/data/stars/starnet_cnn.pb')\n"
     ]
    }
   ],
   "source": [
    "tf_model_path = keras_to_tf(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a denormalization function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_and_std = np.load(denormalization_path)\n",
    "mean_labels = mean_and_std[0]\n",
    "std_labels = mean_and_std[1]\n",
    "num_labels = mean_and_std.shape[1]\n",
    "\n",
    "def denormalize(lb_norm):\n",
    "    return ((lb_norm*std_labels)+mean_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the StarNet model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Test Data **\n",
    "\n",
    "The error propagation technique takes some time, so for the purpose of example, we will only use the first 100 spectra in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set contains 300 stars\n"
     ]
    }
   ],
   "source": [
    "num_test = 300\n",
    "\n",
    "f = h5py.File(test_data_path, 'r')\n",
    "test_spectra = f['spectrum']\n",
    "test_err_spectra = f['error_spectrum']\n",
    "test_ap_ids = f['Ap_ID'][0:num_test]\n",
    "test_labels = np.column_stack((f['TEFF'][0:num_test],f['LOGG'][0:num_test],f['FE_H'][0:num_test]))\n",
    "print('Test set contains '  + str(len(test_ap_ids))+' stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compute predictions and errors for the test set **\n",
    "\n",
    "**Steps:**\n",
    "1. compute predictions\n",
    "\n",
    "    \\begin{equation}\n",
    "    h_(\\textbf{x},\\textbf{W}) =  h_{1}(\\textbf{x},\\textbf{W}),...,h_{j}(\\textbf{x},\\textbf{W}))\n",
    "    \\end{equation} \n",
    "\n",
    "        j = 3\n",
    "\n",
    "2. compute jacobian matrix\n",
    "\n",
    "    \\begin{equation}\n",
    "    Jac = \\frac{\\partial h_{j}(\\textbf{x},\\textbf{W})}{\\partial \\textbf{x}} =  (\\frac{\\partial h_{j}(\\textbf{x},\\textbf{W})}{\\partial x_{1}},...,\\frac{\\partial h_{j}(\\textbf{x},\\textbf{W})}{\\partial x_{n}})\n",
    "    \\end{equation} \n",
    "\n",
    "        j = 1,...,3\n",
    "\n",
    "        n = 7214\n",
    "\n",
    "3. compute covariance matrix\n",
    "\n",
    "    \\begin{equation}\n",
    "    Cov = Jac \\times \\Delta \\textbf{x}^2 \\times Jac^T\n",
    "    \\end{equation}\n",
    "    \n",
    "\n",
    "4. obtain propagated variance due to error spectrum from the diagonal of the covariance matrix\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\sigma_{\\mathrm{prop}}^2 \\approx diag(Cov)\n",
    "    \\end{equation}\n",
    "    \n",
    "\n",
    "5. determine which region of the label-space the labels are within to obtain the intrinsic scatter in the corresponding bin. These values have been predetermined from training StarNet on synthetic data and applying it to a test set of synthetic data\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\sigma_{\\mathrm{int}}\n",
    "    \\end{equation}\n",
    "    \n",
    "6. combine propagated error with the intrinsic scatter term\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\Delta h_{j} = \\sqrt{\\sigma_{\\mathrm{prop}}^2  + \\sigma_{\\mathrm{int}}^2}\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions and computing propagated variance for 300 spectra\n",
      "\n",
      "1 completed.\n",
      "0.773883104324 seconds elapsed.\n",
      "\n",
      "31 completed.\n",
      "21.1932361126 seconds elapsed.\n",
      "\n",
      "61 completed.\n",
      "41.938945055 seconds elapsed.\n",
      "\n",
      "91 completed.\n",
      "62.3968989849 seconds elapsed.\n",
      "\n",
      "121 completed.\n",
      "82.8396980762 seconds elapsed.\n",
      "\n",
      "151 completed.\n",
      "103.467030048 seconds elapsed.\n",
      "\n",
      "181 completed.\n",
      "123.905740023 seconds elapsed.\n",
      "\n",
      "211 completed.\n",
      "144.359602928 seconds elapsed.\n",
      "\n",
      "241 completed.\n",
      "164.856469154 seconds elapsed.\n",
      "\n",
      "271 completed.\n",
      "185.443655968 seconds elapsed.\n",
      "\n",
      "All 300 completed.\n",
      "205.273967028 seconds elapsed.\n"
     ]
    }
   ],
   "source": [
    "variance = np.zeros((len(test_labels),3))\n",
    "predictions = np.zeros(test_labels.shape)\n",
    "print('Making predictions and computing propagated variance for '+str(len(test_labels))+' spectra')\n",
    "time_start = time.time()\n",
    "for i in range(len(test_labels)):\n",
    "    spectrum = test_spectra[i:i+1].reshape((1,7214,1))\n",
    "    err_spectrum = test_err_spectra[i:i+1]\n",
    "    variance[i] = compute_variance_from_tf_model_path(tf_model_path,spectrum,err_spectrum,denormalize)\n",
    "    predictions[i] = denormalize(keras_model.predict(spectrum))\n",
    "    if i%int(0.1*len(test_labels))==0:\n",
    "        print('\\n'+str(i+1)+' completed.\\n'+str(time.time()-time_start)+' seconds elapsed.')\n",
    "print('\\nAll '+str(i+1)+' completed.\\n'+str(time.time()-time_start)+' seconds elapsed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create intrinsic scatter arrays (predetermined) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_terms = np.array([[  2.85209088e+01,   2.30193645e+01,   2.10676180e+01,\n",
    "          1.91357425e+01,   1.72090644e+01,   1.58693655e+01,\n",
    "          1.52684102e+01,   1.42387830e+01,   1.64239293e+01,\n",
    "          2.18981017e+01],\n",
    "       [  3.86073715e-02,   3.04916170e-02,   2.44161726e-02,\n",
    "          2.25093310e-02,   2.35929675e-02,   2.36922221e-02,\n",
    "          2.58764773e-02,   2.80946934e-02,   3.34534390e-02,\n",
    "          3.56641714e-02],\n",
    "       [  3.90793092e-02,   2.43149947e-02,   2.25292707e-02,\n",
    "          1.81974298e-02,   1.58638867e-02,   1.46142515e-02,\n",
    "          1.36038125e-02,   1.25392930e-02,   1.24740228e-02,\n",
    "          1.53680421e-02]])\n",
    "scatter_ranges = np.array([[  3.50000000e+03,   3.95000000e+03,   4.40000000e+03,\n",
    "          4.85000000e+03,   5.30000000e+03,   5.75000000e+03,\n",
    "          6.20000000e+03,   6.65000000e+03,   7.10000000e+03,\n",
    "          7.55000000e+03,   8.00000000e+03],\n",
    "       [  0.00000000e+00,   5.00000000e-01,   1.00000000e+00,\n",
    "          1.50000000e+00,   2.00000000e+00,   2.50000000e+00,\n",
    "          3.00000000e+00,   3.50000000e+00,   4.00000000e+00,\n",
    "          4.50000000e+00,   5.00000000e+00],\n",
    "       [ -2.50000000e+00,  -2.20000000e+00,  -1.90000000e+00,\n",
    "         -1.60000000e+00,  -1.30000000e+00,  -1.00000000e+00,\n",
    "         -7.00000000e-01,  -4.00000000e-01,  -1.00000000e-01,\n",
    "          2.00000000e-01,   5.00000000e-01]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** assign each spectrum an intrinsic scatter term depending on which region of the parameter-space the prediction lies **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scatter_errs = np.empty(test_labels.shape)\n",
    "\n",
    "for i in range(scatter_terms.shape[0]):\n",
    "    for j in range(scatter_terms.shape[1]):\n",
    "        current_min = scatter_ranges[i,j]\n",
    "        current_max = scatter_ranges[i,j+1]\n",
    "        current_scatter = scatter_terms[i,j]\n",
    "        index = np.where((test_labels[:,i]>current_min)&(test_labels[:,i]<current_max))[0]\n",
    "        scatter_errs[index,i]=current_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** combine the propagated error (or the square root of the variance) and intrinsic error in quadrature **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_errors = np.sqrt(variance+np.square(scatter_errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean total statistical errors: \n",
      "\n",
      "Teff  :  30.932 K\n",
      "log(g):  0.064 cgs\n",
      "[Fe/H]:  0.022 dex\n"
     ]
    }
   ],
   "source": [
    "# label names\n",
    "label_names = ['Teff  ','log(g)','[Fe/H]']\n",
    "units = ['K','cgs','dex']\n",
    "\n",
    "mean_err_total = np.mean(total_errors, axis=0)\n",
    "print('Mean total statistical errors: \\n')\n",
    "for i, err in enumerate(mean_err_total):\n",
    "      print(label_names[i]+':  '+\"{0:.3f}\".format(err)+' '+units[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
