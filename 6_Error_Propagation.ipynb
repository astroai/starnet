{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propogate Errors\n",
    "\n",
    "This notebook takes you through the steps of how to propogate errors for through the neural network model\n",
    "\n",
    "* required packages: `numpy h5py keras`\n",
    "* data files: \n",
    "    - starnet_cnn.h5\n",
    "    - mean_and_std.npy\n",
    "    - high_snr_test_data.h5\n",
    "    - apStar_combined_main.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "datadir = '/data/stars/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define path variables for your keras model, denormalization data, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = datadir + 'starnet_cnn.h5'\n",
    "denormalization_path = datadir + 'mean_and_std.npy'\n",
    "test_data_path = datadir + 'high_snr_test_data.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define function to convert the keras model into a tensorflow graph and also a function that can load a tensorflow graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_pb(weight_file,input_fld='',output_fld=''):\n",
    "    \n",
    "    import os\n",
    "    import os.path as osp\n",
    "    from tensorflow.python.framework import graph_util\n",
    "    from tensorflow.python.framework import graph_io\n",
    "    from keras.models import load_model\n",
    "    from keras import backend as K\n",
    "    \n",
    "    \n",
    "    # weight_file is a .h5 keras model file\n",
    "    output_node_names_of_input_network = [\"pred0\"] \n",
    "    output_node_names_of_final_network = 'output_node'\n",
    "    output_graph_name = weight_file[:-2]+'pb'\n",
    "    weight_file_path = osp.join(input_fld, weight_file)\n",
    "    \n",
    "    net_model = load_model(weight_file_path)\n",
    "\n",
    "    num_output = len(output_node_names_of_input_network)\n",
    "    pred = [None]*num_output\n",
    "    pred_node_names = [None]*num_output\n",
    "    for i in range(num_output):\n",
    "        pred_node_names[i] = output_node_names_of_final_network+str(i)\n",
    "        pred[i] = tf.identity(net_model.output[i], name=pred_node_names[i])\n",
    "        \n",
    "    sess = K.get_session()\n",
    "    \n",
    "    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)\n",
    "    graph_io.write_graph(constant_graph, output_fld, output_graph_name, as_text=False)\n",
    "    print('saved the constant graph (ready for inference) at: ', osp.join(output_fld, output_graph_name))\n",
    "    \n",
    "    return output_fld+output_graph_name\n",
    "\n",
    "def load_graph(frozen_graph_filename):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # Then, we can use again a convenient built-in function to import a graph_def into the \n",
    "    # current default Graph\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(\n",
    "            graph_def, \n",
    "            input_map=None, \n",
    "            return_elements=None, \n",
    "            name=\"prefix\", \n",
    "            op_dict=None, \n",
    "            producer_op_list=None\n",
    "        )\n",
    "        \n",
    "    input_name = graph.get_operations()[0].name+':0'\n",
    "    output_name = graph.get_operations()[-1].name+':0'\n",
    "    \n",
    "    return graph, input_name, output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save model as a tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 10 variables.\n",
      "Converted 10 variables to const ops.\n",
      "('saved the constant graph (ready for inference) at: ', '/data/stars/starnet_cnn.pb')\n"
     ]
    }
   ],
   "source": [
    "tf_model_path = convert_to_pb(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a denormalization function **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_and_std = np.load(denormalization_path)\n",
    "mean_labels = mean_and_std[0]\n",
    "std_labels = mean_and_std[1]\n",
    "num_labels = mean_and_std.shape[1]\n",
    "\n",
    "def denormalize(lb_norm):\n",
    "    return ((lb_norm*std_labels)+mean_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the StarNet model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load Test Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    f = h5py.File(filename, 'r')\n",
    "    spectra_array = f['spectrum']\n",
    "    err_spectra_array = f['error_spectrum']\n",
    "    ap_ids = f['Ap_ID'][:]\n",
    "    labels_array = np.column_stack((f['TEFF'][:],f['LOGG'][:],f['FE_H'][:]))\n",
    "    return  (ap_ids, spectra_array, err_spectra_array, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High S/N test set contains 2651 stars\n"
     ]
    }
   ],
   "source": [
    "test_1_ap_ids, test_1_spectra, test_1_err_spectra, test_1_labels = get_data(test_data_path)\n",
    "print('High S/N test set contains '  + str(len(test_1_spectra))+' stars')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load entire APOGEE dataset**\n",
    "\n",
    "This is necessary to obtain the an accurate assessment of the scatter between the model predictions and apogee labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = h5py.File(datadir + 'apStar_combined_main_dr13.h5', 'r')\n",
    "all_apogee_spectra = F['spectrum']\n",
    "all_apogee_labels = np.column_stack((F['TEFF'][:],F['LOGG'][:],F['FE_H'][:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict on entire APOGEE dataset using batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on entire APOGEE database of 148724 objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in divide\n",
      "/opt/conda/lib/python2.7/site-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions complete.\n"
     ]
    }
   ],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 \n",
    "\n",
    "all_apogee_pred = np.zeros((len(all_apogee_spectra),num_labels))\n",
    "\n",
    "batch_size = 500\n",
    "\n",
    "print('Predicting on entire APOGEE database of '+str(len(all_apogee_spectra))+' objects...')\n",
    "\n",
    "for i in range(len(all_apogee_spectra)/batch_size):\n",
    "    \n",
    "    spectra = all_apogee_spectra[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "    # Separate spectra into chips\n",
    "\n",
    "    blue_sp = spectra[:,blue_chip_begin:blue_chip_end]\n",
    "    green_sp = spectra[:,green_chip_begin:green_chip_end]\n",
    "    red_sp = spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "    #Normalize spectra by chips\n",
    "\n",
    "    blue_sp_med = np.median(blue_sp, axis=1)\n",
    "    green_sp_med = np.median(green_sp, axis=1)\n",
    "    red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "    blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "    green_sp = (green_sp.T/green_sp_med).T\n",
    "    red_sp = (red_sp.T/red_sp_med).T  \n",
    "\n",
    "    # Recombine spectra\n",
    "\n",
    "    spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "\n",
    "    # Reshape spectra\n",
    "    spectra = spectra.reshape((spectra.shape[0],spectra.shape[1],1))\n",
    "\n",
    "    all_apogee_pred[i*batch_size:(i+1)*batch_size] = denormalize(keras_model.predict(spectra))\n",
    "    \n",
    "    \n",
    "spectra = all_apogee_spectra[(i+1)*batch_size:]\n",
    "\n",
    "# Separate spectra into chips\n",
    "\n",
    "blue_sp = spectra[:,blue_chip_begin:blue_chip_end]\n",
    "green_sp = spectra[:,green_chip_begin:green_chip_end]\n",
    "red_sp = spectra[:,red_chip_begin:red_chip_end]\n",
    "\n",
    "#Normalize spectra by chips\n",
    "\n",
    "blue_sp_med = np.median(blue_sp, axis=1)\n",
    "green_sp_med = np.median(green_sp, axis=1)\n",
    "red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "green_sp = (green_sp.T/green_sp_med).T\n",
    "red_sp = (red_sp.T/red_sp_med).T  \n",
    "\n",
    "# Recombine spectra\n",
    "\n",
    "spectra = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "\n",
    "# Reshape spectra\n",
    "spectra = spectra.reshape((spectra.shape[0],spectra.shape[1],1))\n",
    "\n",
    "all_apogee_pred[(i+1)*batch_size:] = denormalize(keras_model.predict(spectra))\n",
    "\n",
    "print('Predictions complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate residuals between StarNet predictions and ASPCAP labels**\n",
    "\n",
    "Exclude stars with bad ASPCAP labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.where((all_apogee_labels[:,0]!=-9999.)&(all_apogee_labels[:,1]!=-9999.)&(all_apogee_labels[:,2]!=-9999.))\n",
    "\n",
    "all_apogee_pred = all_apogee_pred[indices]\n",
    "all_apogee_labels = all_apogee_labels[indices]\n",
    "\n",
    "all_apogee_resids = all_apogee_pred - all_apogee_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate residuals into different bins**\n",
    "\n",
    "This is necessary to undestand how the scatter in different ranges of the label-space differs so that the appropriate scatter values are used when including the scatter in the error propogation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resid_t1 = all_apogee_resids[np.where((all_apogee_labels[:,0]<4000)&(all_apogee_labels[:,0]>10))[0]]\n",
    "resid_t2 = all_apogee_resids[np.where((all_apogee_labels[:,0]<4500)&(all_apogee_labels[:,0]>4000))[0]]\n",
    "resid_t3 = all_apogee_resids[np.where((all_apogee_labels[:,0]<4750)&(all_apogee_labels[:,0]>4500))[0]]\n",
    "resid_t4 = all_apogee_resids[np.where((all_apogee_labels[:,0]<5250)&(all_apogee_labels[:,0]>4750))[0]]\n",
    "resid_t5 = all_apogee_resids[np.where((all_apogee_labels[:,0]>5250))[0]]\n",
    "\n",
    "resid_l1 = all_apogee_resids[np.where((all_apogee_labels[:,1]<0.5)&(all_apogee_labels[:,1]>-10.))[0]]\n",
    "resid_l2 = all_apogee_resids[np.where((all_apogee_labels[:,1]<1.5)&(all_apogee_labels[:,1]>0.5))[0]]\n",
    "resid_l3 = all_apogee_resids[np.where((all_apogee_labels[:,1]<2.5)&(all_apogee_labels[:,1]>1.5))[0]]\n",
    "resid_l4 = all_apogee_resids[np.where((all_apogee_labels[:,1]<3.5)&(all_apogee_labels[:,1]>2.5))[0]]\n",
    "resid_l5 = all_apogee_resids[np.where((all_apogee_labels[:,1]>3.5))[0]]\n",
    "\n",
    "resid_f1 = all_apogee_resids[np.where((all_apogee_labels[:,2]<-1.3)&(all_apogee_labels[:,2]>-10.))[0]]\n",
    "resid_f2 = all_apogee_resids[np.where((all_apogee_labels[:,2]<-0.9)&(all_apogee_labels[:,2]>-1.3))[0]]\n",
    "resid_f3 = all_apogee_resids[np.where((all_apogee_labels[:,2]<-0.5)&(all_apogee_labels[:,2]>-0.9))[0]]\n",
    "resid_f4 = all_apogee_resids[np.where((all_apogee_labels[:,2]<-0.1)&(all_apogee_labels[:,2]>-0.5))[0]]\n",
    "resid_f5 = all_apogee_resids[np.where((all_apogee_labels[:,2]>-0.1))[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain a random sample of residuals from each bin**\n",
    "\n",
    "Each sample has to be equal in size for proper statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(resid_t1)\n",
    "resid_t1 = resid_t1[0:1500]\n",
    "np.random.shuffle(resid_t2)\n",
    "resid_t2 = resid_t2[0:1500]\n",
    "np.random.shuffle(resid_t3)\n",
    "resid_t3 = resid_t3[0:1500]\n",
    "np.random.shuffle(resid_t4)\n",
    "resid_t4 = resid_t4[0:1500]\n",
    "np.random.shuffle(resid_t5)\n",
    "resid_t5 = resid_t5[0:1500]\n",
    "\n",
    "np.random.shuffle(resid_l1)\n",
    "resid_l1 = resid_l1[0:1500]\n",
    "np.random.shuffle(resid_l2)\n",
    "resid_l2 = resid_l2[0:1500]\n",
    "np.random.shuffle(resid_l3)\n",
    "resid_l3 = resid_l3[0:1500]\n",
    "np.random.shuffle(resid_l4)\n",
    "resid_l4 = resid_l4[0:1500]\n",
    "np.random.shuffle(resid_l5)\n",
    "resid_l5 = resid_l5[0:1500]\n",
    "\n",
    "np.random.shuffle(resid_f1)\n",
    "resid_f1 = resid_f1[0:1500]\n",
    "np.random.shuffle(resid_f2)\n",
    "resid_f2 = resid_f2[0:1500]\n",
    "np.random.shuffle(resid_f3)\n",
    "resid_f3 = resid_f3[0:1500]\n",
    "np.random.shuffle(resid_f4)\n",
    "resid_f4 = resid_f4[0:1500]\n",
    "np.random.shuffle(resid_f5)\n",
    "resid_f5 = resid_f5[0:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate scatter in different regions, $\\delta_{js}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_resid_t1 = np.std(resid_t1, axis=0)[0]\n",
    "std_resid_t2 = np.std(resid_t2, axis=0)[0]\n",
    "std_resid_t3 = np.std(resid_t3, axis=0)[0]\n",
    "std_resid_t4 = np.std(resid_t4, axis=0)[0]\n",
    "std_resid_t5 = np.std(resid_t5, axis=0)[0]\n",
    "\n",
    "std_resid_l1 = np.std(resid_l1, axis=0)[1]\n",
    "std_resid_l2 = np.std(resid_l2, axis=0)[1]\n",
    "std_resid_l3 = np.std(resid_l3, axis=0)[1]\n",
    "std_resid_l4 = np.std(resid_l4, axis=0)[1]\n",
    "std_resid_l5 = np.std(resid_l5, axis=0)[1]\n",
    "\n",
    "std_resid_f1 = np.std(resid_f1, axis=0)[2]\n",
    "std_resid_f2 = np.std(resid_f2, axis=0)[2]\n",
    "std_resid_f3 = np.std(resid_f3, axis=0)[2]\n",
    "std_resid_f4 = np.std(resid_f4, axis=0)[2]\n",
    "std_resid_f5 = np.std(resid_f5, axis=0)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a function that returns the Jacobian matrix**\n",
    "\n",
    "The jacobian matrix is a matrix of the first order derivatives of the outputs with respect to the input. In our case, this will be a 3-dimensional matrix with dimensions: (num_labels, num_test_spectra, num_flux_values).\n",
    "\n",
    "Each spectrum will therefore have 3 vectors the length of the spectrum: one vector for each of the first order derivatives of the output labels with respect to each flux value (wavelength bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_jacobian(model_path,input_data,denormalize=None):\n",
    "\n",
    "    tf_model,tf_input,tf_output = load_graph(model_path)    \n",
    "    \n",
    "    x = tf_model.get_tensor_by_name(tf_input)\n",
    "    \n",
    "    if denormalize==None:\n",
    "        y = tf_model.get_tensor_by_name(tf_output)\n",
    "    else:\n",
    "        y = denormalize(tf_model.get_tensor_by_name(tf_output))\n",
    "        \n",
    "    y_list = tf.unstack(y)\n",
    "    num_outputs = y.shape.as_list()[0]\n",
    "    \n",
    "    if input_data.shape[0]==1:\n",
    "        with tf.Session(graph=tf_model) as sess:\n",
    "            y_out = sess.run([tf.gradients(y_, x)[0] for y_ in y_list], feed_dict={\n",
    "                x: input_data\n",
    "            })\n",
    "            jacobian = np.asarray(y_out)\n",
    "            jacobian = jacobian[:,:,:,0]\n",
    "    else:\n",
    "        print('\\nCreating jacobian matrices for '+str(len(input_data))+' spectra...\\n')\n",
    "        print_count = int(len(input_data)/10)\n",
    "        if print_count==0:\n",
    "            print_count=1     \n",
    "            \n",
    "        jacobian = np.zeros((num_outputs,input_data.shape[0],input_data.shape[1]))\n",
    "        for i in range(input_data.shape[0]):\n",
    "            with tf.Session(graph=tf_model) as sess:\n",
    "                y_out = sess.run([tf.gradients(y_, x)[0] for y_ in y_list], feed_dict={\n",
    "                    x: input_data[i:i+1]\n",
    "                })\n",
    "            jac_temp = np.asarray(y_out)\n",
    "            jacobian[:,i:i+1,:]=jac_temp[:,:,:,0]\n",
    "            if (i+1)%print_count==0:\n",
    "                print(str(i+1)+' jacobians completed...\\n')\n",
    "        print('All '+str(i+1)+' jacobians completed.\\n')\n",
    "    return jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a function that returns the Covariance Matrix **\n",
    "\n",
    "Within the function, mask extremely high values in the error spectra and nan values in Jacobian\n",
    "\n",
    "The high values in the error spectrum are associated - for the most part - with zero-values in the APOGEE spectra. Since these zero values are essentially ignored in the model (due to RELU-activation and maxpooling layers) they do not effect the output labels and therefore, the flux errors associated with these zero-values give an innaccurate assessment of the prediction errors. If you were to include these error fluxes, you would have massive uncertainties in some of the stars' output labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_covariance(var,jac_matrix):\n",
    "    var[var > 3] = 0\n",
    "    jac_matrix = np.nan_to_num(jac_matrix)\n",
    "    covariance = np.einsum('ijk,kjl->jil',(jac_matrix*(var**2)),jac_matrix.T)\n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Compute Predictions and errors for the test set **\n",
    "**Steps:**\n",
    "1. compute predictions\n",
    "\n",
    "    \\begin{equation}\n",
    "    h_(\\textbf{x},\\textbf{W}) =  h_{1}(\\textbf{x},\\textbf{W}),...,h_{j}(\\textbf{x},\\textbf{W}))\n",
    "    \\end{equation} \n",
    "\n",
    "        j = 3\n",
    "\n",
    "2. compute jacobian matrix\n",
    "\n",
    "    \\begin{equation}\n",
    "    Jac = \\frac{\\partial h_{j}(\\textbf{x},\\textbf{W})}{\\partial \\textbf{x}} =  (\\frac{\\partial h_{j}(\\textbf{x},\\textbf{W})}{\\partial x_{1}},...,\\frac{\\partial h_{j}(\\textbf{x},\\textbf{W})}{\\partial x_{n}})\n",
    "    \\end{equation} \n",
    "\n",
    "        j = 1,...,3\n",
    "\n",
    "        n = 7214\n",
    "\n",
    "3. compute covariance matrix\n",
    "\n",
    "    \\begin{equation}\n",
    "    Cov = Jac \\times \\Delta \\textbf{x}^2 \\times Jac^T\n",
    "    \\end{equation}\n",
    "\n",
    "4. obtain error due to error spectrum from the square root of the diagonal of the covariance matrix\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\delta_{jx} = \\sqrt{diag(Cov)}\n",
    "    \\end{equation}\n",
    "\n",
    "5. determine which region of the label-space the labels are within to obtain the scatter in the corresponding bin\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\delta_{js}\n",
    "    \\end{equation}\n",
    "    \n",
    "6. combine scatter with the error due to the error spectrum\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\Delta h_{j} = \\sqrt{\\delta_{j\\textbf{x}}^{2}  + \\delta_{js}^{2}}\n",
    "    \\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing errors for 2651 objects...\n",
      "0 errors competed...\n",
      "265 errors competed...\n",
      "530 errors competed...\n",
      "795 errors competed...\n",
      "1060 errors competed...\n",
      "1325 errors competed...\n",
      "1590 errors competed...\n",
      "1855 errors competed...\n",
      "2120 errors competed...\n",
      "2385 errors competed...\n",
      "2650 errors competed...\n",
      "Error propagation for 2651 stars completed.\n"
     ]
    }
   ],
   "source": [
    "test_1_predictions = np.zeros((len(test_1_spectra),num_labels))\n",
    "test_1_errors = np.zeros((len(test_1_spectra),num_labels))\n",
    "\n",
    "print('Computing errors for '+str(len(test_1_predictions))+' objects...')\n",
    "\n",
    "for i in range(len(test_1_spectra)):\n",
    "    spectrum = test_1_spectra[i:i+1].reshape(1,7214,1)\n",
    "    label = denormalize(keras_model.predict(spectrum))\n",
    "    test_1_predictions[i] = label\n",
    "    jacobian = compute_jacobian(tf_model_path,spectrum,denormalize)\n",
    "    covariance = compute_covariance(test_1_err_spectra[i:i+1],jacobian)\n",
    "    errors_from_err_spec = np.sqrt(np.diagonal(covariance, offset=0, axis1=1, axis2=2))\n",
    "    label=label.T\n",
    "    std_resid_temp = np.zeros((1,3))\n",
    "    if (label[0]<4000) & (label[0]>10):\n",
    "        std_resid_temp[0,0]=std_resid_t1\n",
    "    elif (label[0]<4500) & (label[0]>4000):\n",
    "        std_resid_temp[0,0]=std_resid_t2\n",
    "    elif (label[0]<4750) & (label[0]>4500):\n",
    "        std_resid_temp[0,0]=std_resid_t3\n",
    "    elif (label[0]<5250) & (label[0]>4750):\n",
    "        std_resid_temp[0,0]=std_resid_t4\n",
    "    elif (label[0]<10000) & (label[0]>5250):\n",
    "        std_resid_temp[0,0]=std_resid_t5\n",
    "        \n",
    "    if (label[1]<0.5) & (label[0]>-10):\n",
    "        std_resid_temp[0,1]=std_resid_l1\n",
    "    elif (label[1]<1.5) & (label[0]>0.5):\n",
    "        std_resid_temp[0,1]=std_resid_l2\n",
    "    elif (label[1]<2.5) & (label[0]>1.5):\n",
    "        std_resid_temp[0,1]=std_resid_l3\n",
    "    elif (label[1]<3.5) & (label[0]>2.5):\n",
    "        std_resid_temp[0,1]=std_resid_l4\n",
    "    elif (label[1]<100) & (label[0]>3.5):\n",
    "        std_resid_temp[0,1]=std_resid_l5\n",
    "    \n",
    "    if (label[2]<-1.3) & (label[0]>-10):\n",
    "        std_resid_temp[0,2]=std_resid_f1\n",
    "    elif (label[2]<-0.9) & (label[0]>-1.3):\n",
    "        std_resid_temp[0,2]=std_resid_f2\n",
    "    elif (label[2]<-0.5) & (label[0]>-0.9):\n",
    "        std_resid_temp[0,2]=std_resid_f3\n",
    "    elif (label[2]<-0.3) & (label[0]>-0.5):\n",
    "        std_resid_temp[0,2]=std_resid_f4\n",
    "    elif (label[2]<0.5) & (label[0]>-0.3):\n",
    "        std_resid_temp[0,2]=std_resid_f5\n",
    "    \n",
    "    if i%int(0.1*len(test_1_spectra))==0:\n",
    "        print(str(i)+' errors competed...')\n",
    "    \n",
    "    test_1_errors[i] = np.sqrt(errors_from_err_spec+np.square(std_resid_temp))\n",
    "    \n",
    "print('Error propagation for '+str(len(test_1_errors))+' stars completed.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean total statistical errors: \n",
      "\n",
      "Teff  :  53.750 K\n",
      "log(g):  0.251 cgs\n",
      "[Fe/H]:  0.162 dex\n"
     ]
    }
   ],
   "source": [
    "# label names\n",
    "label_names = ['Teff  ','log(g)','[Fe/H]']\n",
    "units = ['K','cgs','dex']\n",
    "\n",
    "mean_err_total = np.mean(test_1_errors, axis=0)\n",
    "print('Mean total statistical errors: \\n')\n",
    "for i, err in enumerate(mean_err_total):\n",
    "      print(label_names[i]+':  '+\"{0:.3f}\".format(err)+' '+units[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
