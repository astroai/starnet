{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the StarNet Model\n",
    "\n",
    "This notebook takes you through the steps of how to train a StarNet Model\n",
    "- Required Python packages: `numpy h5py pytorch torchsummary`\n",
    "- Required data files: training_data.h5, mean_and_std.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "datadir = \"\"\n",
    "training_set = datadir + 'training_data.h5'\n",
    "normalization_data = datadir + 'mean_and_std.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization**\n",
    "\n",
    "Write a function to normalize the output labels. Each label will be normalized to have approximately have a mean of zero and unit variance.\n",
    "\n",
    "NOTE: This is necessary to put output labels on a similar scale in order for the model to train properly, this process is reversed in the test stage to give the output labels their proper units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_and_std = np.load(normalization_data)\n",
    "mean_labels = mean_and_std[0]\n",
    "std_labels = mean_and_std[1]\n",
    "\n",
    "def normalize(labels):\n",
    "    # Normalize labels\n",
    "    return (labels-mean_labels) / std_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain training data**\n",
    "\n",
    "Here we will collect the output labels for the training and validation sets, then normalize each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each spectrum contains 7214 wavelength bins\n",
      "Training set includes 41000 spectra and the validation set includes 3784 spectra\n"
     ]
    }
   ],
   "source": [
    "# Define the number of output labels\n",
    "num_labels = np.load(datadir+'mean_and_std.npy').shape[1]\n",
    "\n",
    "# Define the number of training spectra\n",
    "num_train = 41000\n",
    "\n",
    "# Load spectra and labels\n",
    "with  h5py.File(training_set, 'r') as data_F:\n",
    "    x_train = data_F['spectrum'][0:num_train]\n",
    "    x_val = data_F['spectrum'][num_train:]\n",
    "    y_train = np.hstack((data_F['TEFF'][0:num_train], \n",
    "                         data_F['LOGG'][0:num_train], \n",
    "                         data_F['FE_H'][0:num_train]))\n",
    "    y_val = np.hstack((data_F['TEFF'][num_train:], \n",
    "                      data_F['LOGG'][num_train:], \n",
    "                      data_F['FE_H'][num_train:]))\n",
    "\n",
    "# Normalize labels\n",
    "y_train = normalize(y_train)\n",
    "y_val = normalize(y_val)\n",
    "\n",
    "# Define the number of output labels\n",
    "num_labels = y_train.shape[1]\n",
    "num_fluxes = x_train.shape[1]\n",
    "\n",
    "print('Each spectrum contains ' + str(num_fluxes) + ' wavelength bins')\n",
    "print('Training set includes ' + str(x_train.shape[0]) + \n",
    "      ' spectra and the validation set includes ' + str(x_val.shape[0])+' spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the StarNet model architecture**\n",
    "\n",
    "The StarNet architecture is built with:\n",
    "- input layer\n",
    "- 2 convolutional layers\n",
    "- 1 maxpooling layer followed by flattening for the fully connected layer\n",
    "- 2 fully connected layers\n",
    "- output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 4, 7207]              36\n",
      "            Conv1d-2             [-1, 16, 7200]             528\n",
      "         MaxPool1d-3             [-1, 16, 1800]               0\n",
      "            Linear-4                  [-1, 256]       7,373,056\n",
      "            Linear-5                  [-1, 128]          32,896\n",
      "            Linear-6                    [-1, 3]             387\n",
      "================================================================\n",
      "Total params: 7,406,903\n",
      "Trainable params: 7,406,903\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 1.32\n",
      "Params size (MB): 28.26\n",
      "Estimated Total Size (MB): 29.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Number of filters used in the convolutional layers\n",
    "num_filters = [4,16]\n",
    "\n",
    "# Length of the filters in the convolutional layers\n",
    "filter_length = 8\n",
    "\n",
    "# Length of the maxpooling window \n",
    "pool_length = 4\n",
    "\n",
    "# Number of nodes in each of the hidden fully connected layers\n",
    "num_hidden = [256,128]\n",
    "\n",
    "def compute_out_size(in_size, mod):\n",
    "    \"\"\"\n",
    "    Compute output size of Module `mod` given an input with size `in_size`.\n",
    "    \"\"\"\n",
    "    \n",
    "    f = mod.forward(autograd.Variable(torch.Tensor(1, *in_size)))\n",
    "    return f.size()[1:]\n",
    "\n",
    "class StarNet(nn.Module):\n",
    "    def __init__(self, num_fluxes, num_filters, filter_length, \n",
    "                 pool_length, num_hidden, num_labels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional and pooling layers\n",
    "        self.conv1 = nn.Conv1d(1, num_filters[0], filter_length)\n",
    "        self.conv2 = nn.Conv1d(num_filters[0], num_filters[1], filter_length)\n",
    "        self.pool = nn.MaxPool1d(pool_length, pool_length)\n",
    "        \n",
    "        # Determine shape after pooling\n",
    "        pool_output_shape = compute_out_size((1,num_fluxes), \n",
    "                                             nn.Sequential(self.conv1, \n",
    "                                                           self.conv2, \n",
    "                                                           self.pool))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(pool_output_shape[0]*pool_output_shape[1], num_hidden[0])\n",
    "        self.fc2 = nn.Linear(num_hidden[0], num_hidden[1])\n",
    "        self.output = nn.Linear(num_hidden[1], num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = StarNet(num_fluxes, num_filters, filter_length, \n",
    "          pool_length, num_hidden, num_labels)\n",
    "\n",
    "summary(model, (1, num_fluxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More model techniques**\n",
    "* The `Adam` optimizer is the gradient descent algorithm used for minimizing the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of spectra fed into model at once during training\n",
    "batch_size = 32\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 15\n",
    "\n",
    "# initial learning rate for optimization algorithm\n",
    "learning_rate = 0.0007\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate,\n",
    "                             weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(x_train),torch.Tensor(y_train))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(torch.Tensor(x_val),torch.Tensor(y_val))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Train Loss: 0.0635, Val Loss: 0.0178\n",
      "[Epoch 2] Train Loss: 0.0078, Val Loss: 0.0103\n",
      "[Epoch 3] Train Loss: 0.0053, Val Loss: 0.0060\n",
      "[Epoch 4] Train Loss: 0.0047, Val Loss: 0.0067\n",
      "[Epoch 5] Train Loss: 0.0042, Val Loss: 0.0036\n",
      "[Epoch 6] Train Loss: 0.0044, Val Loss: 0.0036\n",
      "[Epoch 7] Train Loss: 0.0035, Val Loss: 0.0038\n",
      "[Epoch 8] Train Loss: 0.0035, Val Loss: 0.0044\n",
      "[Epoch 9] Train Loss: 0.0032, Val Loss: 0.0030\n",
      "[Epoch 10] Train Loss: 0.0031, Val Loss: 0.0056\n",
      "[Epoch 11] Train Loss: 0.0030, Val Loss: 0.0027\n",
      "[Epoch 12] Train Loss: 0.0028, Val Loss: 0.0027\n",
      "[Epoch 13] Train Loss: 0.0026, Val Loss: 0.0026\n",
      "[Epoch 14] Train Loss: 0.0029, Val Loss: 0.0026\n",
      "[Epoch 15] Train Loss: 0.0038, Val Loss: 0.0057\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print_iters = 200\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # Collect batch data\n",
    "        x_batch, y_batch = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        y_pred = model(x_batch.unsqueeze(1))\n",
    "        loss = torch.nn.MSELoss()(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % print_iters == 0:\n",
    "            print('[Epoch %i, %0.0f%%] Train Loss: %0.4f' % (epoch+1, \n",
    "                                                       (i+1)/len(train_dataloader)*100, \n",
    "                                                       running_loss/(i+1)), end=\"\\r\")\n",
    "    train_loss = running_loss/len(train_dataloader)\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_dataloader, 0):\n",
    "            # Collect batch data\n",
    "            x_batch, y_batch = data\n",
    "            y_pred = model(x_batch.unsqueeze(1))\n",
    "            loss = torch.nn.MSELoss()(y_pred, y_batch)\n",
    "            running_loss += loss.item()\n",
    "    val_loss = running_loss/len(val_dataloader)\n",
    "    print('[Epoch %i] Train Loss: %0.4f, Val Loss: %0.4f' % (epoch+1,  \n",
    "                                                             train_loss, \n",
    "                                                             val_loss))\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename =  os.path.join(datadir,'starnet_cnn.pth.tar')\n",
    "torch.save({'optimizer' : optimizer.state_dict(),\n",
    "            'model' : model.state_dict()}, \n",
    "           model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
